Table of Contents

*   [What is semi-supervised learning?](#what-is-semi-supervised-learning)
    
*   [Notations](#notations)
    
*   [Hypotheses](#hypotheses)
    
*   [Consistency Regularization](#consistency-regularization)
    *   [Œ†-model](#%cf%80-model)
        
    *   [Temporal ensembling](#temporal-ensembling)
        
    *   [Mean teachers](#mean-teachers)
        
    *   [Noisy samples as learning targets](#noisy-samples-as-learning-targets)
        
*   [Pseudo Labeling](#pseudo-labeling)
    *   [Label propagation](#label-propagation)
        
    *   [Self-Training](#self-training)
        
    *   [Reducing confirmation bias](#reducing-confirmation-bias)
        
*   [Pseudo Labeling with Consistency Regularization](#pseudo-labeling-with-consistency-regularization)
    *   [MixMatch](#mixmatch)
        
    *   [DivideMix](#dividemix)
        
    *   [FixMatch](#fixmatch)
        
*   [Combined with Powerful Pre-Training](#combined-with-powerful-pre-training)
    
*   [Citation](#citation)
    
*   [References](#references)
    

When facing a limited amount of labeled data for supervised learning tasks, four approaches are commonly discussed.

1.  **Pre-training + fine-tuning**: Pre-train a powerful task-agnostic model on a large unsupervised data corpus, e.g. [pre-training LMs](https://lilianweng.github.io/posts/2019-01-31-lm/)
     on free text, or pre-training vision models on unlabelled images via [self-supervised learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
    , and then fine-tune it on the downstream task with a small set of labeled samples.
2.  **Semi-supervised learning**: Learn from the labelled and unlabeled samples together. A lot of research has happened on vision tasks within this approach.
3.  **Active learning**: Labeling is expensive, but we still want to collect more given a cost budget. Active learning learns to select most valuable unlabeled samples to be collected next and helps us act smartly with a limited budget.
4.  **Pre-training + dataset auto-generation**: Given a capable pre-trained model, we can utilize it to auto-generate a lot more labeled samples. This has been especially popular within the language domain driven by the success of few-shot learning.

I plan to write a series of posts on the topic of ‚ÄúLearning with not enough data‚Äù. Part 1 is on _Semi-Supervised Learning_.

What is semi-supervised learning?[#](#what-is-semi-supervised-learning)

========================================================================

Semi-supervised learning uses both labeled and unlabeled data to train a model.

Interestingly most existing literature on semi-supervised learning focuses on vision tasks. And instead pre-training + fine-tuning is a more common paradigm for language tasks.

All the methods introduced in this post have a loss combining two parts: L\=Ls+Œº(t)Lu. The supervised loss Ls is easy to get given all the labeled examples. We will focus on how the unsupervised loss Lu is designed. A common choice of the weighting term Œº(t) is a ramp function increasing the importance of Lu in time, where t is the training step.

> _Disclaimer_: The post is not gonna cover semi-supervised methods with focus on model architecture modification. Check [this survey](https://arxiv.org/abs/2006.05278)
>  for how to use generative models and graph-based methods in semi-supervised learning.

Notations[#](#notations)

=========================

| Symbol | Meaning |
| --- | --- |
| L | Number of unique labels. |
| (xl,y)‚àºX,y‚àà{0,1}L | Labeled dataset. y is a one-hot representation of the true label. |
| u‚àºU | Unlabeled dataset. |
| D\=X‚à™U | The entire dataset, including both labeled and unlabeled examples. |
| x | Any sample which can be either labeled or unlabeled. |
| x¬Ø | x with augmentation applied. |
| xi | The i\-th sample. |
| L, Ls, Lu | Loss, supervised loss, and unsupervised loss. |
| Œº(t) | The unsupervised loss weight, increasing in time. |
| p(y|x),pŒ∏(y|x) | The conditional probability over the label set given the input. |
| fŒ∏(.) | The implemented neural network with weights Œ∏, the model that we want to train. |
| z\=fŒ∏(x) | A vector of logits output by f. |
| y^\=softmax(z) | The predicted label distribution. |
| D\[.,.\] | A distance function between two distributions, such as MSE, cross entropy, KL divergence, etc. |
| Œ≤ | EMA weighting hyperparameter for [teacher](#mean-teachers)
 model weights. |
| Œ±,Œª | Parameters for MixUp, Œª‚àºBeta(Œ±,Œ±). |
| T | Temperature for sharpening the predicted distribution. |
| œÑ | A confidence threshold for selecting the qualified prediction. |

Hypotheses[#](#hypotheses)

===========================

Several hypotheses have been discussed in literature to support certain design decisions in semi-supervised learning methods.

*   H1: **Smoothness Assumptions**: If two data samples are close in a high-density region of the feature space, their labels should be the same or very similar.
    
*   H2: **Cluster Assumptions**: The feature space has both dense regions and sparse regions. Densely grouped data points naturally form a cluster. Samples in the same cluster are expected to have the same label. This is a small extension of H1.
    
*   H3: **Low-density Separation Assumptions**: The decision boundary between classes tends to be located in the sparse, low density regions, because otherwise the decision boundary would cut a high-density cluster into two classes, corresponding to two clusters, which invalidates H1 and H2.
    
*   H4: **Manifold Assumptions**: The high-dimensional data tends to locate on a low-dimensional manifold. Even though real-world data might be observed in very high dimensions (e.g. such as images of real-world objects/scenes), they actually can be captured by a lower dimensional manifold where certain attributes are captured and similar points are grouped closely (e.g. images of real-world objects/scenes are not drawn from a uniform distribution over all pixel combinations). This enables us to learn a more efficient representation for us to discover and measure similarity between unlabeled data points. This is also the foundation for representation learning. \[see [a helpful link](https://stats.stackexchange.com/questions/66939/what-is-the-manifold-assumption-in-semi-supervised-learning)\
    \].
    

Consistency Regularization[#](#consistency-regularization)

===========================================================

**Consistency Regularization**, also known as **Consistency Training**, assumes that randomness within the neural network (e.g. with Dropout) or data augmentation transformations should not modify model predictions given the same input. Every method in this section has a consistency regularization loss as Lu.

This idea has been adopted in several [self-supervised](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)
 [learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)
 methods, such as SimCLR, BYOL, SimCSE, etc. Different augmented versions of the same sample should result in the same representation. [Cross-view training](https://lilianweng.github.io/posts/2019-01-31-lm/#cross-view-training)
 in language modeling and multi-view learning in self-supervised learning all share the same motivation.

Œ†-model[#](#œÄ-model)

---------------------

![](PI-model.png)

Fig. 1. Overview of the Œ†-model. Two versions of the same input with different stochastic augmentation and dropout masks pass through the network and the outputs are expected to be consistent. (Image source: [Laine & Aila (2017)](https://arxiv.org/abs/1610.02242)
)

[Sajjadi et al. (2016)](https://arxiv.org/abs/1606.04586)
 proposed an unsupervised learning loss to minimize the difference between two passes through the network with stochastic transformations (e.g. dropout, random max-pooling) for the same data point. The label is not explicitly used, so the loss can be applied to unlabeled dataset. [Laine & Aila (2017)](https://arxiv.org/abs/1610.02242)
 later coined the name, **Œ†-Model**, for such a setup.

LuŒ†\=‚àëx‚ààDMSE(fŒ∏(x),fŒ∏‚Ä≤(x))

where f‚Ä≤ is the same neural network with different stochastic augmentation or dropout masks applied. This loss utilizes the entire dataset.

Temporal ensembling[#](#temporal-ensembling)

---------------------------------------------

![](temperal-ensembling.png)

Fig. 2. Overview of Temporal Ensembling. The per-sample EMA label prediction is the learning target. (Image source: [Laine & Aila (2017)](https://arxiv.org/abs/1610.02242)
)

Œ†-model requests the network to run two passes per sample, doubling the computation cost. To reduce the cost, **Temporal Ensembling** ([Laine & Aila 2017](https://arxiv.org/abs/1610.02242)
) maintains an exponential moving average (EMA) of the model prediction in time per training sample z~i as the learning target, which is only evaluated and updated once per epoch. Because the ensemble output z~i is initialized to 0, it is normalized by (1‚àíŒ±t) to correct this startup bias. Adam optimizer has such [bias correction](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for)
 terms for the same reason.

z~i(t)\=Œ±z~i(t‚àí1)+(1‚àíŒ±)zi1‚àíŒ±t

where z~(t) is the ensemble prediction at epoch t and zi is the model prediction in the current round. Note that since z~(0)\=0, with correction, z~(1) is simply equivalent to zi at epoch 1.

Mean teachers[#](#mean-teachers)

---------------------------------

![](mean-teacher.png)

Fig. 3. Overview of the Mean Teacher framework. (Image source: [Tarvaninen & Valpola, 2017](https://arxiv.org/abs/1703.01780)
)

Temporal Ensembling keeps track of an EMA of label predictions for each training sample as a learning target. However, this label prediction only changes _every epoch_, making the approach clumsy when the training dataset is large. **Mean Teacher** ([Tarvaninen & Valpola, 2017](https://arxiv.org/abs/1703.01780)
) is proposed to overcome the slowness of target update by tracking the moving average of model weights instead of model outputs. Let‚Äôs call the original model with weights Œ∏ as the _student_ model and the model with moving averaged weights Œ∏‚Ä≤ across consecutive student models as the _mean teacher_: Œ∏‚Ä≤‚ÜêŒ≤Œ∏‚Ä≤+(1‚àíŒ≤)Œ∏

The consistency regularization loss is the distance between predictions by the student and teacher and the student-teacher gap should be minimized. The mean teacher is expected to provide more accurate predictions than the student. It got confirmed in the empirical experiments, as shown in Fig. 4.

![](mean-teacher-results.png)

Fig. 4. Classification error on SVHN of Mean Teacher and the Œ† Model. The mean teacher (in orange) has better performance than the student model (in blue). (Image source: [Tarvaninen & Valpola, 2017](https://arxiv.org/abs/1703.01780)
)

According to their ablation studies,

*   Input augmentation (e.g. random flips of input images, Gaussian noise) or student model dropout is necessary for good performance. Dropout is not needed on the teacher model.
*   The performance is sensitive to the EMA decay hyperparameter Œ≤. A good strategy is to use a small Œ≤\=0.99 during the ramp up stage and a larger Œ≤\=0.999 in the later stage when the student model improvement slows down.
*   They found that MSE as the consistency cost function performs better than other cost functions like KL divergence.

Noisy samples as learning targets[#](#noisy-samples-as-learning-targets)

-------------------------------------------------------------------------

Several recent consistency training methods learn to minimize prediction difference between the original unlabeled sample and its corresponding augmented version. It is quite similar to the Œ†-model but the consistency regularization loss is _only_ applied to the unlabeled data.

![](consistency-training-with-noisy-samples.png)

Fig. 5. Consistency training with noisy samples.

Adversarial Training ([Goodfellow et al. 2014](https://arxiv.org/abs/1412.6572)
) applies adversarial noise onto the input and trains the model to be robust to such adversarial attack. The setup works in supervised learning,

Ladv(xl,Œ∏)\=D\[q(y‚à£xl),pŒ∏(y‚à£xl+radv)\]radv\=arg‚Å°maxr;‚Äñr‚Äñ‚â§œµD\[q(y‚à£xl),pŒ∏(y‚à£xl+radv)\]radv‚âàœµg‚Äñg‚Äñ2‚âàœµsign(g)where¬†g\=‚àárD\[y,pŒ∏(y‚à£xl+r)\]

where q(y‚à£xl) is the true distribution, approximated by one-hot encoding of the ground truth label, y. pŒ∏(y‚à£xl) is the model prediction. D\[.,.\] is a distance function measuring the divergence between two distributions.

**Virtual Adversarial Training** (**VAT**; [Miyato et al. 2018](https://arxiv.org/abs/1704.03976)
) extends the idea to work in semi-supervised learning. Because q(y‚à£xl) is unknown, VAT replaces it with the current model prediction for the original input with the current weights Œ∏^. Note that Œ∏^ is a fixed copy of model weights, so there is no gradient update on Œ∏^.

LuVAT(x,Œ∏)\=D\[pŒ∏^(y‚à£x),pŒ∏(y‚à£x+rvadv)\]rvadv\=arg‚Å°maxr;‚Äñr‚Äñ‚â§œµD\[pŒ∏^(y‚à£x),pŒ∏(y‚à£x+r)\]

The VAT loss applies to both labeled and unlabeled samples. It is a negative smoothness measure of the current model‚Äôs prediction manifold at each data point. The optimization of such loss motivates the manifold to be smoother.

**Interpolation Consistency Training** (**ICT**; [Verma et al. 2019](https://arxiv.org/abs/1903.03825)
) enhances the dataset by adding more interpolations of data points and expects the model prediction to be consistent with interpolations of the corresponding labels. MixUp ([Zheng et al. 2018](https://arxiv.org/abs/1710.09412)
) operation mixes two images via a simple weighted sum and combines it with label smoothing. Following the idea of MixUp, ICT expects the prediction model to produce a label on a mixup sample to match the interpolation of predictions of corresponding inputs:

mixupŒª(xi,xj)\=Œªxi+(1‚àíŒª)xjp(mixupŒª(y‚à£xi,xj))‚âàŒªp(y‚à£xi)+(1‚àíŒª)p(y‚à£xj)

where Œ∏‚Ä≤ is a moving average of Œ∏, which is a [mean teacher](#mean-teachers)
.

![](ICT.png)

Fig. 6. Overview of Interpolation Consistency Training. MixUp is applied to produce more interpolated samples with interpolated labels as learning targets. (Image source: [Verma et al. 2019](https://arxiv.org/abs/1903.03825)
)

Because the probability of two randomly selected unlabeled samples belonging to different classes is high (e.g. There are 1000 object classes in ImageNet), the interpolation by applying a mixup between two random unlabeled samples is likely to happen around the decision boundary. According to the low-density separation [assumptions](#hypotheses)
, the decision boundary tends to locate in the low density regions.

LuICT\=Eui,uj‚àºUEŒª‚àºBeta(Œ±,Œ±)D\[pŒ∏(y‚à£mixupŒª(ui,uj)),mixupŒª(pŒ∏‚Ä≤(y‚à£ui),pŒ∏‚Ä≤(y‚à£uj)\]

where Œ∏‚Ä≤ is a moving average of Œ∏.

Similar to VAT, **Unsupervised Data Augmentation** (**UDA**; [Xie et al. 2020](https://arxiv.org/abs/1904.12848)
) learns to predict the same output for an unlabeled example and the augmented one. UDA especially focuses on studying how the _‚Äúquality‚Äù_ of noise can impact the semi-supervised learning performance with consistency training. It is crucial to use advanced data augmentation methods for producing meaningful and effective noisy samples. Good data augmentation should produce valid (i.e. does not change the label) and diverse noise, and carry targeted inductive biases.

For images, UDA adopts RandAugment ([Cubuk et al. 2019](https://arxiv.org/abs/1909.13719)
) which uniformly samples augmentation operations available in [PIL](https://pillow.readthedocs.io/en/stable/)
, no learning or optimization, so it is much cheaper than AutoAugment.

![](UDA-image-results.png)

Fig. 7. Comparison of various semi-supervised learning methods on CIFAR-10 classification. Fully supervised Wide-ResNet-28-2 and PyramidNet+ShakeDrop have an error rate of \*\*5.4\*\* and \*\*2.7\*\* respectively when trained on 50,000 examples without RandAugment. (Image source: [Xie et al. 2020](https://arxiv.org/abs/1904.12848)
)

For language, UDA combines back-translation and TF-IDF based word replacement. Back-translation preserves the high-level meaning but may not retain certain words, while TF-IDF based word replacement drops uninformative words with low TF-IDF scores. In the experiments on language tasks, they found UDA to be complementary to transfer learning and representation learning; For example, BERT fine-tuned (i.e. BERTFINETUNE in Fig. 8.) on in-domain unlabeled data can further improve the performance.

![](UDA-language-results.png)

Fig. 8. Comparison of UDA with different initialization configurations on various text classification tasks. (Image source: [Xie et al. 2020](https://arxiv.org/abs/1904.12848)
)

When calculating Lu, UDA found two training techniques to help improve the results.

*   _Low confidence masking_: Mask out examples with low prediction confidence if lower than a threshold œÑ.
*   _Sharpening prediction distribution_: Use a low temperature T in softmax to sharpen the predicted probability distribution.
*   _In-domain data filtration_: In order to extract more in-domain data from a large out-of-domain dataset, they trained a classifier to predict in-domain labels and then retain samples with high confidence predictions as in-domain candidates.

ùüôLuUDA\=1\[maxy‚Ä≤pŒ∏^(y‚Ä≤‚à£x)\>œÑ\]‚ãÖD\[pŒ∏^(sharp)(y‚à£x;T),pŒ∏(y‚à£x¬Ø)\]where¬†pŒ∏^(sharp)(y‚à£x;T)\=exp‚Å°(z(y)/T)‚àëy‚Ä≤exp‚Å°(z(y‚Ä≤)/T)

where Œ∏^ is a fixed copy of model weights, same as in VAT, so no gradient update, and x¬Ø is the augmented data point. œÑ is the prediction confidence threshold and T is the distribution sharpening temperature.

Pseudo Labeling[#](#pseudo-labeling)

=====================================

**Pseudo Labeling** ([Lee 2013](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf)
) assigns fake labels to unlabeled samples based on the maximum softmax probabilities predicted by the current model and then trains the model on both labeled and unlabeled samples simultaneously in a pure supervised setup.

Why could pseudo labels work? Pseudo label is in effect equivalent to _Entropy Regularization_ ([Grandvalet & Bengio 2004](https://papers.nips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html)
), which minimizes the conditional entropy of class probabilities for unlabeled data to favor low density separation between classes. In other words, the predicted class probabilities is in fact a measure of class overlap, minimizing the entropy is equivalent to reduced class overlap and thus low density separation.

![](pseudo-label-segregation.png)

Fig. 9. t-SNE visualization of outputs on MNIST test set by models training (a) without and (b) with pseudo labeling on 60000 unlabeled samples, in addition to 600 labeled data. Pseudo labeling leads to better segregation in the learned embedding space. (Image source: [Lee 2013](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.664.3543&rep=rep1&type=pdf)
)

Training with pseudo labeling naturally comes as an iterative process. We refer to the model that produces pseudo labels as teacher and the model that learns with pseudo labels as student.

Label propagation[#](#label-propagation)

-----------------------------------------

**Label Propagation** ([Iscen et al. 2019](https://arxiv.org/abs/1904.04717)
) is an idea to construct a similarity graph among samples based on feature embedding. Then the pseudo labels are ‚Äúdiffused‚Äù from known samples to unlabeled ones where the propagation weights are proportional to pairwise similarity scores in the graph. Conceptually it is similar to a k-NN classifier and both suffer from the problem of not scaling up well with a large dataset.

![](label-propagation.png)

Fig. 10. Illustration of how Label Propagation works. (Image source: [Iscen et al. 2019](https://arxiv.org/abs/1904.04717)
)

Self-Training[#](#self-training)

---------------------------------

**Self-Training** is not a new concept ([Scudder 1965](https://ieeexplore.ieee.org/document/1053799)
, [Nigram & Ghani CIKM 2000](http://www.kamalnigam.com/papers/cotrain-CIKM00.pdf)
). It is an iterative algorithm, alternating between the following two steps until every unlabeled sample has a label assigned:

*   Initially it builds a classifier on labeled data.
*   Then it uses this classifier to predict labels for the unlabeled data and converts the most confident ones into labeled samples.

[Xie et al. (2020)](https://arxiv.org/abs/1911.04252)
 applied self-training in deep learning and achieved great results. On the ImageNet classification task, they first trained an EfficientNet ([Tan & Le 2019](https://arxiv.org/abs/1905.11946)
) model as teacher to generate pseudo labels for 300M unlabeled images and then trained a larger EfficientNet as student to learn with both true labeled and pseudo labeled images. One critical element in their setup is to have _noise_ during student model training but have no noise for the teacher to produce pseudo labels. Thus their method is called **Noisy Student**. They applied stochastic depth ([Huang et al. 2016](https://arxiv.org/abs/1603.09382)
), dropout and RandAugment to noise the student. Noise is important for the student to perform better than the teacher. The added noise has a compound effect to encourage the model‚Äôs decision making frontier to be smooth, on both labeled and unlabeled data.

A few other important technical configs in noisy student self-training are:

*   The student model should be sufficiently large (i.e. larger than the teacher) to fit more data.
*   Noisy student should be paired with data balancing, especially important to balance the number of pseudo labeled images in each class.
*   Soft pseudo labels work better than hard ones.

Noisy student also improves adversarial robustness against an FGSM (Fast Gradient Sign Attack = The attack uses the gradient of the loss w.r.t the input data and adjusts the input data to maximize the loss) attack though the model is not optimized for adversarial robustness.

SentAugment, proposed by [Du et al. (2020)](https://arxiv.org/abs/2010.02194)
, aims to solve the problem when there is not enough in-domain unlabeled data for self-training in the language domain. It relies on sentence embedding to find unlabeled in-domain samples from a large corpus and uses the retrieved sentences for self-training.

Reducing confirmation bias[#](#reducing-confirmation-bias)

-----------------------------------------------------------

Confirmation bias is a problem with incorrect pseudo labels provided by an imperfect teacher model. Overfitting to wrong labels may not give us a better student model.

To reduce confirmation bias, [Arazo et al. (2019)](https://arxiv.org/abs/1908.02983)
 proposed two techniques. One is to adopt MixUp with soft labels. Given two samples, (xi,xj) and their corresponding true or pseudo labels (yi,yj), the interpolated label equation can be translated to a cross entropy loss with softmax outputs:

x¬Ø\=Œªxi+(1‚àíŒª)xjy¬Ø\=Œªyi+(1‚àíŒª)yj‚áîL\=Œª\[yi‚ä§log‚Å°fŒ∏(x¬Ø)\]+(1‚àíŒª)\[yj‚ä§log‚Å°fŒ∏(x¬Ø)\]

Mixup is insufficient if there are too few labeled samples. They further set a minimum number of labeled samples in every mini batch by oversampling the labeled samples. This works better than upweighting labeled samples, because it leads to more frequent updates rather than few updates of larger magnitude which could be less stable. Like consistency regularization, data augmentation and dropout are also important for pseudo labeling to work well.

**Meta Pseudo Labels** ([Pham et al. 2021](https://arxiv.org/abs/2003.10580)
) adapts the teacher model constantly with the feedback of how well the student performs on the labeled dataset. The teacher and the student are trained in parallel, where the teacher learns to generate better pseudo labels and the student learns from the pseudo labels.

Let the teacher and student model weights be Œ∏T and Œ∏S, respectively. The student model‚Äôs loss on the labeled samples is defined as a function Œ∏SPL(.) of Œ∏T and we would like to minimize this loss by optimizing the teacher model accordingly.

minŒ∏TLs(Œ∏SPL(Œ∏T))\=minŒ∏TE(xl,y)‚ààXCE\[y,fŒ∏S(xl)\]where¬†Œ∏SPL(Œ∏T)\=arg‚Å°minŒ∏SLu(Œ∏T,Œ∏S)\=arg‚Å°minŒ∏SEu‚àºUCE\[(fŒ∏T(u),fŒ∏S(u))\]

However, it is not trivial to optimize the above equation. Borrowing the idea of [MAML](https://arxiv.org/abs/1703.03400)
, it approximates the multi-step arg‚Å°minŒ∏S with the one-step gradient update of Œ∏S,

Œ∏SPL(Œ∏T)‚âàŒ∏S‚àíŒ∑S‚ãÖ‚àáŒ∏SLu(Œ∏T,Œ∏S)minŒ∏TLs(Œ∏SPL(Œ∏T))‚âàminŒ∏TLs(Œ∏S‚àíŒ∑S‚ãÖ‚àáŒ∏SLu(Œ∏T,Œ∏S))

With soft pseudo labels, the above objective is differentiable. But if using hard pseudo labels, it is not differentiable and thus we need to use RL, e.g. REINFORCE.

The optimization procedure is alternative between training two models:

*   _Student model update_: Given a batch of unlabeled samples {u}, we generate pseudo labels by fŒ∏T(u) and optimize Œ∏S with one step SGD: Œ∏S‚Ä≤\=Œ∏S‚àíŒ∑S‚ãÖ‚àáŒ∏SLu(Œ∏T,Œ∏S).
*   _Teacher model update_: Given a batch of labeled samples {(xl,y)}, we reuse the student‚Äôs update to optimize Œ∏T: Œ∏T‚Ä≤\=Œ∏T‚àíŒ∑T‚ãÖ‚àáŒ∏TLs(Œ∏S‚àíŒ∑S‚ãÖ‚àáŒ∏SLu(Œ∏T,Œ∏S)). In addition, the UDA objective is applied to the teacher model to incorporate consistency regularization.

![](MPL-results.png)

Fig. 11. Comparison of Meta Pseudo Labels with other semi- or self-supervised learning methods on image classification tasks. (Image source: [Pham et al. 2021](https://arxiv.org/abs/2003.10580)
)

Pseudo Labeling with Consistency Regularization[#](#pseudo-labeling-with-consistency-regularization)

=====================================================================================================

It is possible to combine the above two approaches together, running semi-supervised learning with both pseudo labeling and consistency training.

MixMatch[#](#mixmatch)

-----------------------

**MixMatch** ([Berthelot et al. 2019](https://arxiv.org/abs/1905.02249)
), as a holistic approach to semi-supervised learning, utilizes unlabeled data by merging the following techniques:

1.  _Consistency regularization_: Encourage the model to output the same predictions on perturbed unlabeled samples.
2.  _Entropy minimization_: Encourage the model to output confident predictions on unlabeled data.
3.  _MixUp_ augmentation: Encourage the model to have linear behaviour between samples.

Given a batch of labeled data X and unlabeled data U, we create augmented versions of them via MixMatch(.), X¬Ø and U¬Ø, containing augmented samples and guessed labels for unlabeled examples.

X¬Ø,U¬Ø\=MixMatch(X,U,T,K,Œ±)LsMM\=1|X¬Ø|‚àë(x¬Øl,y)‚ààX¬ØD\[y,pŒ∏(y‚à£x¬Øl)\]LuMM\=1L|U¬Ø|‚àë(u¬Ø,y^)‚ààU¬Ø‚Äñy^‚àípŒ∏(y‚à£u¬Ø)‚Äñ22

where T is the sharpening temperature to reduce the guessed label overlap; K is the number of augmentations generated per unlabeled example; Œ± is the parameter in MixUp.

For each u, MixMatch generates K augmentations, u¬Ø(k)\=Augment(u) for k\=1,‚Ä¶,K and the pseudo label is guessed based on the average: y^\=1K‚àëk\=1KpŒ∏(y‚à£u¬Ø(k)).

![](MixMatch.png)

Fig. 12. The process of "label guessing" in MixMatch: averaging K augmentations, correcting the predicted marginal distribution and finally sharpening the distribution. (Image source: [Berthelot et al. 2019](https://arxiv.org/abs/1905.02249)
)

According to their ablation studies, it is critical to have MixUp especially on the unlabeled data. Removing temperature sharpening on the pseudo label distribution hurts the performance quite a lot. Average over multiple augmentations for label guessing is also necessary.

**ReMixMatch** ([Berthelot et al. 2020](https://arxiv.org/abs/1911.09785)
) improves MixMatch by introducing two new mechanisms:

![](ReMixMatch.png)

Fig. 13. Illustration of two improvements introduced in ReMixMatch over MixMatch. (Image source: [Berthelot et al. 2020](https://arxiv.org/abs/1911.09785)
)

*   _Distribution alignment._ It encourages the marginal distribution p(y) to be close to the marginal distribution of the ground truth labels. Let p(y) be the class distribution in the true labels and p~(y^) be a running average of the predicted class distribution among the unlabeled data. The model prediction on an unlabeled sample pŒ∏(y|u) is normalized to be Normalize(pŒ∏(y|u)p(y)p~(y^)) to match the true marginal distribution.
    *   Note that entropy minimization is not a useful objective if the marginal distribution is not uniform.
    *   I do feel the assumption that the class distributions on the labeled and unlabeled data should match is too strong and not necessarily to be true in the real-world setting.
*   _Augmentation anchoring_. Given an unlabeled sample, it first generates an ‚Äúanchor‚Äù version with weak augmentation and then averages K strongly augmented versions using CTAugment (Control Theory Augment). CTAugment only samples augmentations that keep the model predictions within the network tolerance.

The ReMixMatch loss is a combination of several terms,

*   a supervised loss with data augmentation and MixUp applied;
*   an unsupervised loss with data augmentation and MixUp applied, using pseudo labels as targets;
*   a CE loss on a single heavily-augmented unlabeled image without MixUp;
*   a [rotation](https://lilianweng.github.io/posts/2019-11-10-self-supervised/#distortion)
     loss as in self-supervised learning.

DivideMix[#](#dividemix)

-------------------------

**DivideMix** ([Junnan Li et al. 2020](https://arxiv.org/abs/2002.07394)
) combines semi-supervised learning with Learning with noisy labels (LNL). It models the per-sample loss distribution via a [GMM](https://scikit-learn.org/stable/modules/mixture.html)
 to dynamically divide the training data into a labeled set with clean examples and an unlabeled set with noisy ones. Following the idea in [Arazo et al. 2019](https://arxiv.org/abs/1904.11238)
, they fit a two-component GMM on the per-sample cross entropy loss ‚Ñìi\=yi‚ä§log‚Å°fŒ∏(xi). Clean samples are expected to get lower loss faster than noisy samples. The component with smaller mean is the cluster corresponding to clean labels and let‚Äôs denote it as c. If the GMM posterior probability wi\=pGMM(c‚à£‚Ñìi) (i.e. the probability of the sampling belonging to the clean sample set) is larger than the threshold œÑ, this sample is considered as a clean sample and otherwise a noisy one.

The data clustering step is named _co-divide_. To avoid confirmation bias, DivideMix simultaneously trains two diverged networks where each network uses the dataset division from the other network; e.g. thinking about how Double Q Learning works.

![](DivideMix.png)

Fig. 14. DivideMix trains two networks independently to reduce confirmation bias. They run co-divide, co-refinement, and co-guessing together. (Image source: [Junnan Li et al. 2020](https://arxiv.org/abs/2002.07394)
)

Compared to MixMatch, DivideMix has an additional _co-divide_ stage for handling noisy samples, as well as the following improvements during training:

*   _Label co-refinement_: It linearly combines the ground-truth label yi with the network‚Äôs prediction y^i, which is averaged across multiple augmentations of xi, guided by the clean set probability wi produced by the other network.
*   _Label co-guessing_: It averages the predictions from two models for unlabelled data samples.

![](DivideMix-algo.png)

Fig. 15. The algorithm of DivideMix. (Image source: [Junnan Li et al. 2020](https://arxiv.org/abs/2002.07394)
)

FixMatch[#](#fixmatch)

-----------------------

**FixMatch** ([Sohn et al. 2020](https://arxiv.org/abs/2001.07685)
) generates pseudo labels on unlabeled samples with weak augmentation and only keeps predictions with high confidence. Here both weak augmentation and high confidence filtering help produce high-quality trustworthy pseudo label targets. Then FixMatch learns to predict these pseudo labels given a heavily-augmented sample.

![](FixMatch.png)

Fig. 16. Illustration of how FixMatch works. (Image source: [Sohn et al. 2020](https://arxiv.org/abs/2001.07685)
)

ùüôLs\=1B‚àëb\=1BCE\[yb,pŒ∏(y‚à£Aweak(xb))\]Lu\=1ŒºB‚àëb\=1ŒºB1\[max(y^b)‚â•œÑ\]CE(y^b,pŒ∏(y‚à£Astrong(ub)))

where y^b is the pseudo label for an unlabeled example; Œº is a hyperparameter that determines the relative sizes of X and U.

*   Weak augmentation Aweak(.): A standard flip-and-shift augmentation
*   Strong augmentation Astrong(.) : AutoAugment, Cutout, RandAugment, CTAugment

![](FixMatch-results.png)

Fig. 17. Performance of FixMatch and several other semi-supervised learning methods on image classification tasks. (Image source: [Sohn et al. 2020](https://arxiv.org/abs/2001.07685)
)

According to the ablation studies of FixMatch,

*   Sharpening the predicted distribution with a temperature parameter T does not have a significant impact when the threshold œÑ is used.
*   Cutout and CTAugment as part of strong augmentations are necessary for good performance.
*   When the weak augmentation for label guessing is replaced with strong augmentation, the model diverges early in training. If discarding weak augmentation completely, the model overfit the guessed labels.
*   Using weak instead of strong augmentation for pseudo label prediction leads to unstable performance. Strong data augmentation is critical.

Combined with Powerful Pre-Training[#](#combined-with-powerful-pre-training)

=============================================================================

It is a common paradigm, especially in language tasks, to first pre-train a task-agnostic model on a large unsupervised data corpus via self-supervised learning and then fine-tune it on the downstream task with a small labeled dataset. Research has shown that we can obtain extra gain if combining semi-supervised learning with pretraining.

[Zoph et al. (2020)](https://arxiv.org/abs/2006.06882)
 studied to what degree [self-training](#self-training)
 can work better than pre-training. Their experiment setup was to use ImageNet for pre-training or self-training to improve COCO. Note that when using ImageNet for self-training, it discards labels and only uses ImageNet samples as unlabeled data points. [He et al. (2018)](https://arxiv.org/abs/1811.08883)
 has demonstrated that ImageNet classification pre-training does not work well if the downstream task is very different, such as object detection.

![](self-training-pre-training.png)

Fig. 18. The effect of (a) data augment (from weak to strong) and (b) the labeled dataset size on the object detection performance. In the legend: \`Rand Init\` refers to a model initialized w/ random weights; \`ImageNet\` is initialized with a pre-trained checkpoint at 84.5% top-1 ImageNet accuracy; \`ImageNet++\` is initialized with a checkpoint with a higher accuracy 86.9%. (Image source: [Zoph et al. 2020](https://arxiv.org/abs/2006.06882)
)

Their experiments demonstrated a series of interesting findings:

*   The effectiveness of pre-training diminishes with more labeled samples available for the downstream task. Pre-training is helpful in the low-data regimes (20%) but neutral or harmful in the high-data regime.
*   Self-training helps in high data/strong augmentation regimes, even when pre-training hurts.
*   Self-training can bring in additive improvement on top of pre-training, even using the same data source.
*   Self-supervised pre-training (e.g. via SimCLR) hurts the performance in a high data regime, similar to how supervised pre-training does.
*   Joint-training supervised and self-supervised objectives help resolve the mismatch between the pre-training and downstream tasks. Pre-training, joint-training and self-training are all additive.
*   Noisy labels or un-targeted labeling (i.e. pre-training labels are not aligned with downstream task labels) is worse than targeted pseudo labeling.
*   Self-training is computationally more expensive than fine-tuning on a pre-trained model.

[Chen et al. (2020)](https://arxiv.org/abs/2006.10029)
 proposed a three-step procedure to merge the benefits of self-supervised pretraining, supervised fine-tuning and self-training together:

1.  Unsupervised or self-supervised pretrain a big model.
2.  Supervised fine-tune it on a few labeled examples. It is important to use a big (deep and wide) neural network. _Bigger models yield better performance with fewer labeled samples._
3.  Distillation with unlabeled examples by adopting pseudo labels in self-training.
    *   It is possible to distill the knowledge from a large model into a small one because the task-specific use does not require extra capacity of the learned representation.
    *   The distillation loss is formatted as the following, where the teacher network is fixed with weights Œ∏^T.

Ldistill\=‚àí(1‚àíŒ±)‚àë(xil,yi)‚ààX\[log‚Å°pŒ∏S(yi‚à£xil)\]‚èüSupervised loss‚àíŒ±‚àëui‚ààU\[‚àëi\=1LpŒ∏^T(y(i)‚à£ui;T)log‚Å°pŒ∏S(y(i)‚à£ui;T)\]‚èüDistillation loss using unlabeled data

![](big-self-supervised-model.png)

Fig. 19. A semi-supervised learning framework leverages unlabeled data corpus by (Left) task-agnostic unsupervised pretraining and (Right) task-specific self-training and distillation. (Image source: [Chen et al. 2020](https://arxiv.org/abs/2006.10029)
)

They experimented on the ImageNet classification task. The self-supervised pre-training uses SimCLRv2, a directly improved version of [SimCLR](https://lilianweng.github.io/posts/2021-05-31-contrastive/#simclr)
. Observations in their empirical studies confirmed several learnings, aligned with [Zoph et al. 2020](https://arxiv.org/abs/2006.06882)
:

*   Bigger models are more label-efficient;
*   Bigger/deeper project heads in SimCLR improve representation learning;
*   Distillation using unlabeled data improves semi-supervised learning.

![](big-self-supervised-model-results.png)

Fig. 20. Comparison of performance by SimCLRv2 + semi-supervised distillation on ImageNet classification. (Image source: [Chen et al. 2020](https://arxiv.org/abs/2006.10029)
)

* * *

üí° Quick summary of common themes among recent semi-supervised learning methods, many aiming to reduce confirmation bias:

*   Apply valid and diverse noise to samples by advanced data augmentation methods.
*   When dealing with images, MixUp is an effective augmentation. Mixup could work on language too, resulting in a small incremental improvement ([Guo et al. 2019](https://arxiv.org/abs/1905.08941)
    ).
*   Set a threshold and discard pseudo labels with low confidence.
*   Set a minimum number of labeled samples per mini-batch.
*   Sharpen the pseudo label distribution to reduce the class overlap.

