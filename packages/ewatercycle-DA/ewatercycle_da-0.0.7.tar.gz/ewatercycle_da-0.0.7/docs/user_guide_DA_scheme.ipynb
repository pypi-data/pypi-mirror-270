{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cd8d85-eb4a-45d6-925a-e3e770592043",
   "metadata": {},
   "source": [
    "# Tutorial: Implementing your own Data Assimilation Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3061420-d276-4f3e-ba8e-c0ca73aece58",
   "metadata": {},
   "source": [
    "As part of the Data Assimilation framework, classes which apply the assimilation steps are purposely kept seperate as shown in the [design flowchart](https://ewatercycle-da.readthedocs.io/en/latest/_images/method1_define_upfront.png).<br>\n",
    "This allows users to implement their own schemes to assimilate models. <br>\n",
    "Currently (april 2024) these must be implemented in the same  file as DA, but this is just a pointer to a class. <br>\n",
    "You could with little modification change this (we might later actually). <br>\n",
    "Regradless of where it lives, the implementation is still the same. <br> \n",
    "This tutorial will walk you through what you need to have in order to create a working framework. <br>\n",
    "The framework has been designed to work with: \n",
    "- Particle Filters (PF)\n",
    "- Ensemble Kalman Filter (EnKF)\n",
    "- Ensemble Smoothers (ES)\n",
    "- Ensemble Smoother Multiple Data Assimilation (ES-MDA)\n",
    "- Iterative Ensemble Smoother (IES)\n",
    "  \n",
    "Others will likely work, but no guarantees. 4Dvar likely won't easily work as this requires time to be a parameter, which adds a whole layer of complexity. \n",
    "To explain how the scheme should be strucutred a `NewDataAssimilationScheme` class will be constructed here to show the different steps required.\n",
    "\n",
    "eWaterCycle in general uses [pydantic](https://docs.pydantic.dev/latest/) for validation, thus you inherit the `BaseModel` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e86823-368b-4f76-b4a2-8607deeb9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class NewDataAssimilationScheme(BaseModel):\n",
    "    \"\"\"Implementation of a new DA scheme\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69074c-38e0-47e9-8eb2-8629f8fe5479",
   "metadata": {},
   "source": [
    "The class has one Argument: \n",
    " - `N` which is set when initialising the ensemble\n",
    "The class has a 5 required attributes:\n",
    " - `hyperparameters` (dictionary): Combination of many different parameters which you might need. Can also be empty if not needed ofcourse\n",
    " - `obs` (float): observation value of the current model timestep, set in due course thus optional\n",
    " - `state_vectors` (np.ndarray): current state vector (i.e. input) per ensemble member with shape N x len(z), where z is the state vector. \n",
    " - `predictions` (np.ndarray): contains prior modeled values per ensemble member, was assumed  N x 1 but dependant on what you pass as H opperator.\n",
    " - `new_state_vectors` (np.ndarray): updated state vector (i.e. output) per ensemble member & thus shape N x len(z)\n",
    "\n",
    "Then add in any optional attributes you may want to access for debugging or other reasons:\n",
    "- new_data_assimilation_specific_var (...): example of what you might add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "163769cd-8826-4d4d-9b36-64a9046efa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "class NewDataAssimilationScheme(BaseModel):\n",
    "    \"\"\"Implementation of a new DA scheme\"\"\"\n",
    "    N: int\n",
    "    \n",
    "    hyperparameters: dict = dict()\n",
    "    obs: float | Any | None = None\n",
    "    state_vectors: Any | None = None\n",
    "    predictions: Any | None = None\n",
    "    new_state_vectors: Any | None = None\n",
    "    \n",
    "    new_data_assimilation_specific_var: Any | None = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd18284-9f24-4349-b287-931f0fae633a",
   "metadata": {},
   "source": [
    "The class requires the functon `update` which assimilates the current state vector and sets the new state vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56f68ec9-a993-4707-aa20-393936f9e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDataAssimilationScheme(BaseModel):\n",
    "    \"\"\"Implementation of a new DA scheme\"\"\"\n",
    "    N: int\n",
    "    hyperparameters: dict = dict()\n",
    "    obs: float | Any | None = None\n",
    "    state_vectors: Any | None = None\n",
    "    predictions: Any | None = None\n",
    "    new_state_vectors: Any | None = None\n",
    "    new_data_assimilation_specific_var: Any | None = None\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Takes current state vectors of ensemble and returns updated state vectors ensemble\"\"\"\n",
    "        state_vector = self.state_vectors\n",
    "\n",
    "        # ... \n",
    "        # insert scheme\n",
    "        # ... \n",
    "\n",
    "        self.new_state_vectors = state_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a74eff-0abd-4cd1-8a19-27fee8adc1e9",
   "metadata": {},
   "source": [
    "As mentioned, the DA schemes are currently purposely controlled to avoid misuse/confusion. <br>\n",
    "For this reasons if you implement as scheme you must add it to the `LOADED_METHODS` dictionary.<br>\n",
    "Each of the different DA schemes have their own `.py` files under `ewatercycle_DA.data_assimilation_schemes`.\n",
    "The current method looks like this: \n",
    "```py\n",
    "    ...\n",
    "    \n",
    "    from ewatercycle_DA.data_assimilation_schemes.PF import ParticleFilter\r",
    "    \n",
    "from ewatercycle_DA.data_assimilation_schemes.EnKF import EnsembleKalmanFilte\n",
    "    r\n",
    "    LOADED_METHODS: dict[str, Any] = dict(\n",
    "                                            PF=ParticleFilter,\n",
    "                                            EnKF=EnsembleKalmanFilter,\n",
    "                                         \n",
    "    ...)\n",
    "\n",
    "```\n",
    "\n",
    "Because everything in DA is shortend, adding our new method will be:\n",
    "\n",
    "```py\n",
    "    ...\n",
    "    \n",
    "    from ewatercycle_DA.data_assimilation_schemes.NDAS import NewDataAssimilationScheme\n",
    "    \n",
    "    ...\n",
    "\n",
    "    LOADED_METHODS: dict[str, Any] = dict(\n",
    "                                            PF=ParticleFilter,\n",
    "                                            EnKF=EnsembleKalmanFilter,\n",
    "                                            NDAS=NewDataAssimilationScheme\n",
    "                                         )\n",
    "    ...\n",
    "\n",
    "```\n",
    "This is if you put the file called `NDAS.py` file in the folder `data_assimilation_schemes` with the class `NewDataAssimilationScheme`.\n",
    "\n",
    "_advanced:_ <br>\n",
    "This dictionary merely holds a pointer to the class: you could also host the class somewhere else and provide it: \n",
    "```py\n",
    "    from ewatercycle-NewDataAssimilationScheme import NewDataAssimilationScheme\n",
    "    new_scheme_method = {'NDAS': NewDataAssimilationScheme}\n",
    "    LOADED_METHODS.update(new_scheme_method)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204a5c9-09fd-4f64-b92a-30416535e039",
   "metadata": {},
   "source": [
    "As an example, the current ParticelFilter class looks like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "346cba31-cf6b-4298-a947-11272fcc21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleFilter(BaseModel):\n",
    "    \"\"\"Implementation of a particle filter scheme to be applied to the :py:class:`Ensemble`.\n",
    "\n",
    "    note:\n",
    "        The :py:class:`ParticleFilter` is controlled by the :py:class:`Ensemble` and thus has no time reference itself.\n",
    "        No DA method should need to know where in time it is (for now).\n",
    "        Currently assumed 1D grid.\n",
    "\n",
    "    Args:\n",
    "        N (int): Size of ensemble, passed down from DA.Ensemble().\n",
    "\n",
    "    Attributes:\n",
    "        hyperparameters (dict): Combination of many different parameters:\n",
    "                                like_sigma_weights (float): scale/sigma of logpdf when generating particle weights\n",
    "\n",
    "                                like_sigma_state_vector (float): scale/sigma of noise added to each value in state vector\n",
    "\n",
    "        obs (float): observation value of the current model timestep, set in due course thus optional\n",
    "\n",
    "        state_vectors (np.ndarray): state vector per ensemble member [N x len(z)]\n",
    "\n",
    "        predictions (np.ndarray): contains prior modeled values per ensemble member [N x 1]\n",
    "\n",
    "        new_state_vectors (np.ndarray): updated state vector per ensemble member [N x len(z)]\n",
    "\n",
    "        weights (np.ndarray): contains weights per ensemble member per prior modeled values [N x 1]\n",
    "\n",
    "        resample_indices (np.ndarray): contains indices of particles that are resampled [N x 1]\n",
    "\n",
    "\n",
    "    All are :obj:`None` by default\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # args\n",
    "    N: int\n",
    "\n",
    "    # required attributes\n",
    "    hyperparameters: dict = dict(like_sigma_weights=0.05, like_sigma_state_vector=0.0005)\n",
    "    obs: float | Any | None = None # TODO: refactor to np.ndarray\n",
    "    state_vectors: Any | None = None\n",
    "    predictions: Any | None = None\n",
    "    new_state_vectors: Any | None = None\n",
    "\n",
    "    # extra attributes\n",
    "    weights: Any | None = None\n",
    "    resample_indices: Any | None = None\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Takes current state vectors of ensemble and returns updated state vectors ensemble\n",
    "        \"\"\"\n",
    "        self.generate_weights()\n",
    "\n",
    "        # TODO: Refactor to be more modular i.e. remove if/else\n",
    "\n",
    "        # 1d for now: weights is N x 1\n",
    "        if self.weights[0].size == 1:\n",
    "            self.resample_indices = random.choices(population=np.arange(self.N), weights=self.weights, k=self.N)\n",
    "\n",
    "            new_state_vectors = self.state_vectors.copy()[self.resample_indices]\n",
    "            new_state_vectors_transpose = new_state_vectors.T # change to len(z) x N so in future you can vary sigma\n",
    "\n",
    "            # for now just constant perturbation, can vary this hyperparameter\n",
    "            like_sigma = self.hyperparameters['like_sigma_state_vector']\n",
    "            if type(like_sigma) is float:\n",
    "                for index, row in enumerate(new_state_vectors_transpose):\n",
    "                    row_with_noise = np.array([s + add_normal_noise(like_sigma) for s in row])\n",
    "                    new_state_vectors_transpose[index] = row_with_noise\n",
    "\n",
    "            elif type(like_sigma) is list and len(like_sigma) == len(new_state_vectors_transpose):\n",
    "                for index, row in enumerate(new_state_vectors_transpose):\n",
    "                    row_with_noise = np.array([s + add_normal_noise(like_sigma[index]) for s in row])\n",
    "                    new_state_vectors_transpose[index] = row_with_noise\n",
    "            else:\n",
    "                raise RuntimeWarning(f\"{like_sigma} should be float or list of length {len(new_state_vectors_transpose)}\")\n",
    "\n",
    "\n",
    "\n",
    "            self.new_state_vectors = new_state_vectors_transpose.T # back to N x len(z) to be set correctly\n",
    "\n",
    "        # 2d weights is N x len(z)\n",
    "        else:\n",
    "            # handel each row separately:\n",
    "            self.resample_indices = []\n",
    "            for i in range(len(self.weights[0])):\n",
    "                 self.resample_indices.append(random.choices(population=np.arange(self.N), weights=self.weights[:, i], k=self.N))\n",
    "            self.resample_indices = np.vstack(self.resample_indices)\n",
    "\n",
    "            new_state_vectors_transpose = self.state_vectors.copy().T\n",
    "            for index, indices in enumerate(self.resample_indices):\n",
    "                new_state_vectors_transpose[index] = new_state_vectors_transpose[index, indices]\n",
    "\n",
    "            # for now just constant perturbation, can vary this hyperparameter\n",
    "            like_sigma = self.hyperparameters['like_sigma_state_vector']\n",
    "            for index, row in enumerate(new_state_vectors_transpose):\n",
    "                row_with_noise = np.array([s + add_normal_noise(like_sigma) for s in row])\n",
    "                new_state_vectors_transpose[index] = row_with_noise\n",
    "\n",
    "            self.new_state_vectors = new_state_vectors_transpose.T  # back to N x len(z) to be set correctly\n",
    "\n",
    "\n",
    "\n",
    "    def generate_weights(self):\n",
    "        \"\"\"Takes the ensemble and observations and returns the posterior\"\"\"\n",
    "\n",
    "        like_sigma = self.hyperparameters['like_sigma_weights']\n",
    "        difference = (self.obs - self.predictions)\n",
    "        unnormalised_log_weights = scipy.stats.norm.logpdf(difference, loc=0, scale=like_sigma)\n",
    "        normalised_weights = np.exp(unnormalised_log_weights - scipy.special.logsumexp(unnormalised_log_weights))\n",
    "\n",
    "        self.weights = normalised_weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
