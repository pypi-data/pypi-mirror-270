Metadata-Version: 2.1
Name: mllm
Version: 0.1.1
Summary: Yet another prompt
License: Apache 2.0
Author: Patrick Barker
Author-email: patrickbarkerco@gmail.com
Requires-Python: >=3.10,<4.0
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: litellm (>=1.35.29,<2.0.0)
Requires-Dist: tenacity (>=8.2.3,<9.0.0)
Requires-Dist: threadmem (>=0.2.24,<0.3.0)
Description-Content-Type: text/markdown

# yapr

yapr (Yet Another Prompt)

## Installation

```sh
pip install yapr
```

llms?

## Usage

Create an LLM provider from the API keys found in the current system env vars

```python
from yapr import LLMProvider, RoleThread

llm_provider = LLMProvider.from_env()
```

Create a new role based chat thread

```python
thread = RoleThread()
thread.post(role="user", msg="How are you?")
```

Chat with the LLM, store the prompt data in the namespace "foo"

```python
response = llm_provider.chat(thread, namespace="foo")

thread.add_msg(response.msg)
```

Ask for a structured response

```python
from pydantic import BaseModel

class Foo(BaseModel):
    bar: str
    baz: int

thread.post(role="user", msg="Given the {...} can you return that in JSON?")

response = llm_provider.chat(thread, namespace="foo", response_schema=Foo)
foo_parsed = response.parsed

assert type(foo_parsed) == Foo
```

Multimodal

```python

```

Find a saved thread

```python

```

Find a saved prompt

```python

```

Just store prompts

```python
from yapr import Prompt, RoleThread

thread = RoleThread()

msg = {
    "role": "user",
    "content": [
        {
            "type": "text",
            "text": "Whats in this image?",
        },
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,..."},
        }
    ]
}
role_message = RoleMessage.from_openai(msg)
thread.add_msg(role_message)

response = call_openai(thread.to_openai())
response_msg = RoleMessage.from_openai(response["choices"][0]["message"])

saved_prompt = Prompt(thread, response_msg, namespace="foo")
```

