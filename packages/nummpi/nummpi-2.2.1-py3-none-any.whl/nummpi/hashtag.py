# -*- coding: utf-8 -*-
"""sma_4_hashtag_prac_exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tjgLZWyBT3QIFuLYtpsPtTRFEjfGfBpt
"""

# Exp 4

'''
To determine which hashtags are most popular among different user groups, we can use the following approach:

- Read in the CSV file containing the social media data and filter it to include only the columns we need (e.g. user ID, user group, and hashtags).
- Split the hashtags into individual words and count their frequency for each user group.
- Plot the top N most frequent hashtags for each user group in a bar chart.
'''

import pandas as pd
import matplotlib.pyplot as plt

# Read in the CSV file
df = pd.read_csv('hashtag_analysis.csv')

def extract_hashtags(text):
    # Split by space and only take words that start with '#'
    return [word for word in text.split() if word.startswith('#')]

# Apply the function to the 'hashtags' column to convert it into a list of hashtags
df['hashtags'] = df['hashtags'].apply(extract_hashtags)

# Explode the DataFrame to create a new row for each hashtag
df = df.explode('hashtags')

# Group the data by hashtags and user groups, and count the occurrences
hashtags_by_group = df.groupby(
    ['user_group', 'hashtags']).size().reset_index(name='count')

# Plot a horizontal bar chart for each user group
for group in df['user_group'].unique():
    group_data = hashtags_by_group[hashtags_by_group['user_group'] == group].sort_values('count', ascending=False).head(10)
    ax = group_data.plot(kind='barh', x='hashtags', y='count',
                         legend=False, title=f'Top Hashtags for {group}')
    ax.set_xlabel('Frequency')
    plt.tight_layout()
    plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
import numpy as np

# Read data
df = pd.read_csv('hashtag_analysis.csv')

# Function to extract hashtags
def extract_hashtags(text):
    # Split by space and only take words that start with '#'
    return ' '.join([word for word in text.split() if word.startswith('#')])

# Apply the function to create a list of hashtags for each row
df['hashtags_split'] = df['hashtags'].apply(extract_hashtags)

# Use CountVectorizer to create a matrix of hashtag counts (Bag of Words)
vectorizer = CountVectorizer()
hashtag_matrix = vectorizer.fit_transform(df['hashtags_split'])

# Number of clusters for K-means
num_clusters = 5  # Adjust as desired

# Apply K-means clustering
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df['cluster'] = kmeans.fit_predict(hashtag_matrix)

# Display cluster assignment
print("Cluster assignments for each row:")
print(df[['hashtags', 'cluster']])

# Analyze hashtags in each cluster
for cluster_num in range(num_clusters):
    cluster_hashtags = []
    # Collect hashtags for each cluster
    cluster_data = df[df['cluster'] == cluster_num]['hashtags_split']
    for text in cluster_data:
        cluster_hashtags.extend(text.split())

    unique_hashtags = np.unique(cluster_hashtags)
    print(f"Cluster {cluster_num} Hashtags: {unique_hashtags}")