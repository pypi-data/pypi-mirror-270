# -*- coding: utf-8 -*-
"""SC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14DuVtlh1bhRNOF4J2Vgo2ERn5EQiZx-3

##Practical 01
Implement Fuzzy Relations. (MaxMin Composition)
"""
def f1():
    m1 = [[0.7, 0.6], [0.8, 0.3]]
    m2 = [[0.8, 0.5, 0.4], [0.1, 0.6, 0.7]]

    result = [[0, 0, 0], [0, 0, 0]]

    for i in range(len(m1)):
        for j in range(len(m2[0])):
            result[i][j] = max(min(m1[i][0], m2[0][j]), min(m1[i][1], m2[1][j]))

    for i in result:
        print(i)
    


# """##Practical 02
# Implementation of McCulloch-Pitts Model
# """

# class McCullochPittsNeuron:
#     def __init__(self, weights, threshold):
#         self.weights = weights
#         self.threshold = threshold

#     def activate(self, inputs):
#         weighted_sum = sum(x for x in inputs)
#         return weighted_sum >= self.threshold

# if __name__ == "__main__":
#     weights = [1, 1]
#     threshold = 1.0

#     neuron = McCullochPittsNeuron(weights, threshold)

#     input1 = [1, 1]
#     input2 = [0, 1]
#     input3 = [1, 0]
#     input4 = [0, 0]

#     output1 = neuron.activate(input1)
#     output2 = neuron.activate(input2)
#     output3 = neuron.activate(input3)
#     output4 = neuron.activate(input4)

#     print(f"Input {input1}: Sum {sum(input1)} Output {output1}")
#     print(f"Input {input2}: Sum {sum(input2)} Output {output2}")
#     print(f"Input {input3}: Sum {sum(input3)} Output {output3}")
#     print(f"Input {input4}: Sum {sum(input4)} Output {output4}")

# """##Practical 03
# Implement Single Layer Perceptron Learning Algorithm
# """

# import numpy as np
# from sklearn.datasets import load_iris
# from sklearn.linear_model import Perceptron
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import accuracy_score

# iris = load_iris()
# X = iris.data
# y = iris.target

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

# perceptron = Perceptron()

# perceptron.fit(X_train, y_train)

# y_pred = perceptron.predict(X_test)

# accuracy = accuracy_score(y_test, y_pred)
# print("Accuracy: {:.2f}%".format(accuracy * 100))
# print("Learned Weights :", perceptron.coef_)
# print("Learned bias :", perceptron.intercept_)

# """##Practical 04
# Write a Program for Error Back Propagation Algorithms
# """

# import numpy as np

# def sigmoid(x):
#     return 1 / (1 + np.exp(-x))

# def sigmoid_derivative(x):
#     return x * (1 - x)

# def backpropagation(X, y, num_epochs, learning_rate):
#     # Initialize random weights
#     np.random.seed(1)
#     weights_1 = 2 * np.random.random((3, 4)) - 1
#     weights_2 = 2 * np.random.random((4, 1)) - 1

#     # Training loop
#     for epoch in range(num_epochs):
#         # Forward propagation
#         layer_1 = sigmoid(np.dot(X, weights_1))
#         layer_2 = sigmoid(np.dot(layer_1, weights_2))

#         # Back propagation
#         error = y - layer_2
#         delta_layer_2 = error * sigmoid_derivative(layer_2)
#         delta_layer_1 = delta_layer_2.dot(weights_2.T) * sigmoid_derivative(layer_1)

#         # Update weights
#         weights_2 += learning_rate * layer_1.T.dot(delta_layer_2)
#         weights_1 += learning_rate * X.T.dot(delta_layer_1)

#     return layer_2

# # Test the backpropagation algorithm
# X = np.array([[0, 0, 1],
#               [0, 1, 1],
#               [1, 0, 1],
#               [1, 1, 1]])

# y = np.array([[0],
#               [1],
#               [1],
#               [0]])

# num_epochs = 10000
# learning_rate = 0.1

# output = backpropagation(X, y, num_epochs, learning_rate)
# print(output)

# """##Practical 05
# Write a program to implement RNN
# """

# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# from keras.models import Sequential
# from keras.layers import Dense, SimpleRNN

# # Generate a synthetic dataset
# N = 1000
# Tp = 800
# t = np.arange(0, N)
# x = np.sin(0.02 * t) + 2 * np.random.rand(N)
# df = pd.DataFrame(x)

# # Prepare the dataset
# values = df.values
# train, test = values[0:Tp, :], values[Tp:N, :]

# # Add step elements into train and test
# step = 4
# test = np.append(test, np.repeat(test[-1, :], step))
# train = np.append(train, np.repeat(train[-1, :], step))

# # Convert into dataset matrix
# def convertToMatrix(data, step):
#     X, Y = [], []
#     for i in range(len(data) - step):
#         d = i + step
#         X.append(data[i:d,])
#         Y.append(data[d,])
#     return np.array(X), np.array(Y)

# trainX, trainY = convertToMatrix(train, step)
# testX, testY = convertToMatrix(test, step)

# # Reshape the data
# trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
# testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

# # Build the RNN model
# model = Sequential()
# model.add(SimpleRNN(units=32, input_shape=(1, step), activation="relu"))
# model.add(Dense(8, activation="relu"))
# model.add(Dense(1))
# model.compile(loss='mean_squared_error', optimizer='rmsprop')
# model.summary()

# # Train the model
# model.fit(trainX, trainY, epochs=100, batch_size=16, verbose=2)

# # Make predictions
# trainPredict = model.predict(trainX)
# testPredict = model.predict(testX)
# predicted = np.concatenate((trainPredict, testPredict), axis=0)

# # Plot the results with customizations
# plt.figure(figsize=(10, 6)) # Adjust figure size for better readability
# plt.plot(df.index.values, df.values, label='Original Data', color='blue', linestyle='-')
# plt.plot(df.index.values, predicted, label='Predicted Data', color='red', linestyle='--')
# plt.axvline(df.index[Tp], color='green', linestyle='--', label='Train/Test Split')

# # Customize the plot for better readability
# plt.title('RNN Prediction vs Original Data')
# plt.xlabel('Time Steps')
# plt.ylabel('Values')
# plt.legend()
# plt.grid(True) # Add grid for better visualization
# plt.show()

# """##Practical 06
# Write a program for Image Classification using CNN
# """

# import tensorflow as tf
# from keras import datasets, layers, models

# (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# # Normalize pixel values between 0 and 1
# train_images, test_images = train_images / 255.0, test_images / 255.0

# # Print the shapes of the loaded data
# print("Shape of training images:", train_images.shape)
# print("Shape of training labels:", train_labels.shape)
# print("Shape of test images:", test_images.shape)
# print("Shape of test labels:", test_labels.shape)

# model = models.Sequential()
# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
# model.add(layers.MaxPooling2D((2, 2)))
# model.add(layers.Conv2D(64, (3, 3), activation='relu'))
# model.add(layers.MaxPooling2D((2, 2)))
# model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# # Flatten the output from 2D to 1D
# model.add(layers.Flatten())
# model.add(layers.Dense(64, activation='relu'))
# model.add(layers.Dense(10))  # 10 classes for CIFAR-10

# # Print a summary of the model architecture
# model.summary()

# model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
# history = model.fit(train_images, train_labels, epochs=50, validation_data=(test_images, test_labels))

# test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
# print('Test accuracy:', test_acc)

# model.save('model.h5')

# import numpy as np
# import matplotlib.pyplot as plt
# import cv2
# from google.colab.patches import cv2_imshow

# # Load the trained model
# model = tf.keras.models.load_model('model.h5')

# # Load and preprocess a test image
# image_path = '/content/drive/MyDrive/Dataset/ImageSet/dog1.jpg'
# image = tf.keras.preprocessing.image.load_img(image_path, target_size=(32, 32))
# input_array = tf.keras.preprocessing.image.img_to_array(image)
# input_array = np.expand_dims(input_array, axis=0)
# input_array = input_array / 255.0

# img = cv2.imread(image_path)
# cv2_imshow(img)

# # Make predictions
# predictions = model.predict(input_array)
# predicted_class = np.argmax(predictions[0])

# # Print the predicted class and confidence value
# class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
# print("Predicted class:", class_names[predicted_class])

# """##Practical 07
# Implementation of Simple Genetic Algorithm
# """

# import numpy as np

# # Define the fitness function
# def fitness_function(solution):
#     return np.sum(solution)

# # Initialize population
# population_size = 10
# solution_length = 10
# population = np.random.randint(0, 2, (population_size, solution_length))

# # Main loop
# for generation in range(100):
#     # Calculate fitness for each solution
#     fitnesses = np.array([fitness_function(sol) for sol in population])

#     # Select parents
#     parents = population[np.argsort(fitnesses)[-2:]]

#     # Crossover
#     crossover_point = np.random.randint(0, solution_length)
#     child1 = np.concatenate((parents[0, :crossover_point], parents[1, crossover_point:]))
#     child2 = np.concatenate((parents[1, :crossover_point], parents[0, crossover_point:]))

#     # Mutation
#     mutation_rate = 0.1
#     child1 = np.where(np.random.rand(solution_length) < mutation_rate, 1 - child1, child1)
#     child2 = np.where(np.random.rand(solution_length) < mutation_rate, 1 - child2, child2)

#     # Replacement
#     population[np.argsort(fitnesses)[-2:]] = np.array([child1, child2])

# # Print the best solution
# best_solution = population[np.argmax(fitnesses)]

# print("Best solution:", best_solution)

# """##Practical 08
# Design a fuzzy controller
# """


# import numpy as np
# import skfuzzy as fuzz
# from skfuzzy import control as ctrl

# # Define fuzzy sets for the input (distance to obstacle) and output (speed)
# distance = ctrl.Antecedent(np.arange(0, 101, 1), 'distance')
# speed = ctrl.Consequent(np.arange(0, 101, 1), 'speed')

# # Define fuzzy sets for distance
# distance['close'] = fuzz.trimf(distance.universe, [0, 0, 20])
# distance['medium'] = fuzz.trimf(distance.universe, [20, 40, 60])
# distance['far'] = fuzz.trimf(distance.universe, [60, 80, 100])

# # Define fuzzy sets for speed
# speed['slow'] = fuzz.trimf(speed.universe, [0, 0, 20])
# speed['medium'] = fuzz.trimf(speed.universe, [20, 40, 60])
# speed['fast'] = fuzz.trimf(speed.universe, [60, 80, 100])

# # Define fuzzy rules
# rule1 = ctrl.Rule(distance['close'], speed['fast'])
# rule2 = ctrl.Rule(distance['medium'], speed['medium'])
# rule3 = ctrl.Rule(distance['far'], speed['slow'])

# # Create the fuzzy control system
# fuzzy_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])
# fuzzy_sim = ctrl.ControlSystemSimulation(fuzzy_ctrl)

# # Example input: distance to obstacle
# distance_to_obstacle = 10

# # Input the distance to the fuzzy system
# fuzzy_sim.input['distance'] = distance_to_obstacle

# # Compute the output
# fuzzy_sim.compute()

# # Print the output speed
# print(fuzzy_sim.output['speed'])