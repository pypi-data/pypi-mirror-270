{
    "Header": {
        "title": "aiFlows",
        "paragraph": "The building blocks of your collaborative AI",
        "code": "pip install aiflows",
        "language": "bash",
        "showLineNumbers": false,
        "codeBGColor": "#2d2d2d"
    },

    "Features": [{
            "icon": "fa fa-solid fa-puzzle-piece",
            "title": "Modularity",
            "text": "Design Flows of complex interactions involving humans, AI systems, and tools, like LEGO blocks."
        },
        {
            "icon": "fa fa-regular fa-handshake",
            "title": "Reusability",
            "text": "Readily reuse, customize, and build on top of Flows published on FlowVerse by the community."
        },
        {
            "icon": "fa fa-regular fa-solid fa-arrows-split-up-and-left",
            "title": "Concurrency",
            "text": "Build on top of a concurrency friendly abstraction inspired by the Actor model of concurrent computation."
        },
        {
            "icon": "fa fa-solid fa-hand-holding-heart",
            "title": "Forever Free",
            "text": "The aiFlows library is free and always will be."
        }
    ],
    "About": {
        "paragraph": "Flows is a framework to orchestrate interactions between LLMs, tools and humans. It is designed to be modular and composable, allowing you to build complex reasoning processes from simple building blocks. The Flow-verse allows you to share your flows with the community, and re-use and combine flows created by other researchers.",
        "Why": [
            "Easy to get started: Create your own Flow in just a few lines of code",
            "Flexible: Anything can be a Flow. With our library you can easily let your LLMs talk to other LLMs, tools, and even humans. Our tutorials show convenient templates that you can adjust to your needs.",
            "Modular and composable: Chain together multiple Flows with smart branching and merging of data. Create deeply nested Flows to build complex reasoning processes from simple building blocks.",
            "Seamlessly extends existing libraries, such as LangChain."
        ],
        "Why2": [
            "Caching: Save time and resources by caching the results of your Flows. Our caching mechanism makes your Flows resilient to crashes and interruptions.",
            "Share data: Our cache is based on an intelligent data and task driven hashing mechanism. You can share cached computations across deeply nested Flow architectures.",
            "Multi-threaded: Our Flow execution engine is multi-threaded and offers a powerful waiting&retrying mechanism to integrate with external services.",
            "Visualization toolkit: Debug and visualize the reasoning process of your LLMs."
        ]
    },
    "GettingStarted": {
        "title": "Flow",
        "text_1": "ChatAtomicFlow",
        "code_1": "import os\nimport aiflows\nfrom aiflows.utils import colink_utils, serving\nfrom aiflows import workers\nfrom aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n\ndependencies = [\n    {\"url\": \"aiflows/ChatFlowModule\", \"revision\": \"main\"}\n]\naiflows.flow_verse.sync_dependencies(dependencies)\n\nif __name__ == \"__main__\":\n    #1. ~~~ Start local colink server ~~~\n    cl = colink_utils.start_colink_server()\n    #2. ~~~ Load flow config ~~~\n    root_dir = \"flow_modules/aiflows/ChatFlowModule\"\n    cfg = read_yaml_file(os.path.join(root_dir, \"demo.yaml\"))\n    ##2.1 ~~~ Set the API information ~~~\n    api_information = [aiflows.backends.api_info.ApiInfo(backend_used=\"openai\",\n                                                         api_key=os.getenv(\"OPENAI_API_KEY\"))]\n    quick_load_api_keys(cfg, api_information, key=\"api_infos\")\n    #3. ~~~~ Serve The Flow ~~~~\n    flow_class_name=\"flow_modules.aiflows.ChatFlowModule.ChatAtomicFlow\"\n    serving.serve_flow(\n        cl=cl, flow_class_name=flow_class_name, flow_endpoint=\"ChatAtomicFlow\"\n    )\n    #4. ~~~~~Start A Worker Thread, Mount the Flow and Get its Proxy~~~~~\n    workers.run_dispatch_worker_thread(cl)\n    proxy_flow= serving.get_flow_instance(\n        cl=cl, flow_endpoint=\"ChatAtomicFlow\", user_id=\"local\", config_overrides=cfg\n    )\n    #5. ~~ Get the data ~~\n    data = {\"id\": 0, \"question\": \"Who won the 2023 NBA Championship?\"}\n    input_message = proxy_flow.package_input_message(data = data)\n    #6. ~~~ Run Inference ~~~\n    future = proxy_flow.get_reply_future(input_message)\n    print(future.get_data())\n\n",
        "text_2": "ReAct",
        "code_2": "import os\nimport aiflows\nfrom aiflows.utils import colink_utils, serving\nfrom aiflows import workers\nfrom aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n\ndependencies = [\n    {\"url\": \"aiflows/ControllerExecutorFlowModule\", \"revision\": \"main\"}\n]\naiflows.flow_verse.sync_dependencies(dependencies)\n\nif __name__ == \"__main__\":\n    #1. ~~~ Start local colink server ~~~\n    cl = colink_utils.start_colink_server()\n    #2. ~~~ Load flow config ~~~\n    root_dir = \"flow_modules/aiflows/ControllerExecutorFlowModule\"\n    cfg = read_yaml_file(os.path.join(root_dir, \"demo.yaml\"))\n    ##2.1 ~~~ Set the API information ~~~\n    api_information = [aiflows.backends.api_info.ApiInfo(backend_used=\"openai\",\n                                                         api_key=os.getenv(\"OPENAI_API_KEY\"))]\n    quick_load_api_keys(cfg, api_information, key=\"api_infos\")\n    #3. ~~~~ Serve The Flow ~~~~\n    flow_class_name=\"flow_modules.aiflows.ControllerExecutorFlowModule.ControllerExecutorFlow\"\n    serving.recursive_serve_flow(\n        cl=cl, flow_class_name=flow_class_name, flow_endpoint=\"ControllerExecutorFlow\"\n    )\n    #4. ~~~~~Start A Worker Thread, Mount the Flow and Get its Proxy~~~~~\n    workers.run_dispatch_worker_thread(cl)\n    proxy_flow= serving.get_flow_instance(\n        cl=cl, flow_endpoint=\"ControllerExecutorFlow\", user_id=\"local\", config_overrides=cfg\n    )\n    #5. ~~ Get the data ~~\n    data = {\"id\": 0, \"goal\": \"Who won the 2023 NBA Championship?\"}\n    input_message = proxy_flow.package_input_message(data = data)\n    #6. ~~~ Run Inference ~~~\n    future = proxy_flow.get_reply_future(input_message)\n    print(future.get_data())\n\n",
        "text_3": "AutoGPT",
        "code_3": "import os\nimport aiflows\nfrom aiflows.utils import colink_utils, serving\nfrom aiflows import workers\nfrom aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n\ndependencies = [\n    {\"url\": \"aiflows/AutoGPTFlowModule\", \"revision\": \"main\"},\n    {\"url\": \"aiflows/LCToolFlowModule\", \"revision\": \"main\"}\n]\naiflows.flow_verse.sync_dependencies(dependencies)\n\nif __name__ == \"__main__\":\n    #1. ~~~ Start local colink server ~~~\n    cl = colink_utils.start_colink_server()\n    #2. ~~~ Load flow config ~~~\n    root_dir = \"flow_modules/aiflows/AutoGPTFlowModule\"\n    cfg = read_yaml_file(os.path.join(root_dir, \"demo.yaml\"))\n    ##2.1 ~~~ Set the API information ~~~\n    api_information = [aiflows.backends.api_info.ApiInfo(backend_used=\"openai\",\n                                                         api_key=os.getenv(\"OPENAI_API_KEY\"))]\n    quick_load_api_keys(cfg, api_information, key=\"api_infos\")\n    #3. ~~~~ Serve The Flow ~~~~\n    flow_class_name=\"flow_modules.aiflows.AutoGPTFlowModule.AutoGPTFlow\"\n    serving.recursive_serve_flow(\n        cl=cl, flow_class_name=flow_class_name, flow_endpoint=\"AutoGPTFlow\"\n    )\n    #4. ~~~~~Start A Worker Thread, Mount the Flow and Get its Proxy~~~~~\n    workers.run_dispatch_worker_thread(cl)\n    proxy_flow= serving.get_flow_instance(\n        cl=cl, flow_endpoint=\"AutoGPTFlow\", user_id=\"local\", config_overrides=cfg\n    )\n    #5. ~~ Get the data ~~\n    data = {\"id\": 0, \"goal\": \"Who won the 2023 NBA Championship?\"}\n    input_message = proxy_flow.package_input_message(data = data)\n    #6. ~~~ Run Inference ~~~\n    future = proxy_flow.get_reply_future(input_message)\n    print(future.get_data())",
        "language": "python",
        "showLineNumbers": false
    },
    "Contribute": [{
            "icon": "fa fa-magnifying-glass",
            "name": "Research",
            "text": "The ability to implement arbitrarily complex interactions, paired with complete control and systematicity, make aiFlows an ideal tool for research."
        },
        {
            "icon": "fa fa-solid fa-screwdriver-wrench",
            "name": "Build",
            "text": "FlowVerse brings all research advancements at your fingertips, and aiFlows enables you to build applications and make each Flow your own."
        },
        {
            "icon": "fa fa-solid fa-comment",
            "name": "Critique",
            "text": "Use aiFlows in your personal and professional projects, and push the library to its limits. Let us know what breaks and what can be improved â€“ we are always listening."
        }
    ],
    "Testimonials": [{
            "img": "img/testimonials/01.jpg",
            "text": "\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sed dapibus leo nec ornare diam sedasd commodo nibh ante facilisis bibendum dolor feugiat at.\"",
            "name": "John Doe"
        },
        {
            "img": "img/testimonials/02.jpg",
            "text": "\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sed dapibus leo nec ornare diam sedasd commodo nibh ante facilisis bibendum dolor feugiat at.\"",
            "name": "Johnathan Doe"
        },
        {
            "img": "img/testimonials/03.jpg",
            "text": "\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sed dapibus leo nec ornare diam sedasd commodo nibh ante facilisis bibendum dolor feugiat at.\"",
            "name": "John Doe"
        },
        {
            "img": "img/testimonials/04.jpg",
            "text": "\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sed dapibus leo nec ornare diam sedasd commodo nibh ante facilisis bibendum dolor feugiat at.\"",
            "name": "Johnathan Doe"
        },
        {
            "img": "img/testimonials/05.jpg",
            "text": "\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sed dapibus leo nec ornare diam sedasd commodo nibh ante facilisis bibendum dolor feugiat at.\"",
            "name": "John Doe"
        },
        {
            "img": "img/testimonials/06.jpg",
            "text": "\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sed dapibus leo nec ornare diam sedasd commodo nibh ante facilisis bibendum dolor feugiat at.\"",
            "name": "Johnathan Doe"
        }
    ],
    "Team": [{
            "img": "img/team/lab_logo.png",
            "name": "Martin Josifoski",
            "job": "Phd student"
        },
        {
            "img": "img/team/lab_logo.png",
            "name": "Lars Henning Klein",
            "job": "Phd Student"
        },
        {
            "img": "img/team/lab_logo.png",
            "name": "Maxime Peyrard",
            "job": "Phd Student"
        },
        {
            "img": "img/team/lab_logo.png",
            "name": "Julian Paul Schnitzler",
            "job": "Master student"
        },
        {
            "img": "img/team/lab_logo.png",
            "name": "Yuxing Yao",
            "job": "Master student"
        },
        {
            "img": "img/team/lab_logo.png",
            "name": "Debjit Paul",
            "job": "Postdoc"
        }
    ],
    "Contact": {
        "address": "Rte Cantonale, 1015 Lausanne",
        "phone": "+1 123 456 1234",
        "email": "info@company.com",
        "twitter": "twitter.com",
        "youtube": "youtube.com"
    }
}