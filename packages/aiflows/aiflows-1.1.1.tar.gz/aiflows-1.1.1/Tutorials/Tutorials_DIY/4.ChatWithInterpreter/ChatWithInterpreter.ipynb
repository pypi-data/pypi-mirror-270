{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3\n",
    "\n",
    "**In this tutorial you will:**\n",
    "- Build a Chatbot connected to a code interpreter able to execute Python code [Section 1](#1-chatbotwithinterpreter-expanding-interactivity-and-functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#imports\n",
    "from aiflows.utils.general_helpers import read_yaml_file, quick_load_api_keys\n",
    "from aiflows.backends.api_info import ApiInfo\n",
    "from aiflows.utils import serving\n",
    "from aiflows.utils import colink_utils\n",
    "from aiflows.workers import run_dispatch_worker_thread\n",
    "from aiflows.base_flows import AtomicFlow\n",
    "from aiflows.messages import FlowMessage\n",
    "from aiflows import flow_verse\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from utils import compile_and_writefile, dict_to_yaml\n",
    "import json\n",
    "import copy\n",
    "#Specify path of your flow modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting a local colink server\n",
    "cl = colink_utils.start_colink_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Worker thread\n",
    "run_dispatch_worker_thread(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start 2nd Worker thread (in case you're making blocking calls)\n",
    "run_dispatch_worker_thread(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ChatBotWithInterpreter: Expanding Interactivity and Functionality\n",
    "\n",
    "In this section, we'll explore the implementation of a chatbot integrated with a code interpreter, offering new dimensions of interactivity and functionality. By connecting an interpreter capable of compiling and executing code to the chatbot, we unlock numerous benefits. This setup enables users to prompt the chatbot with specific tasks, allowing them to provide instructions like \"download Apple's stock prices, plot them, and save them in a PDF.\" In response, the chatbot generates tailored code to execute these tasks, and executes it. \n",
    "\n",
    "Additionally, integrating the interpreter enhances the chatbot's adaptability and versatility. By understanding the user's environment, such as their Python environment, the chatbot can execute code seamlessly within that context. This capability extends to installing necessary Python packages on-the-fly, ensuring smooth execution without requiring user intervention. Such integration enhances the chatbot's utility, making it a versatile tool for coding assistance, troubleshooting, and rapid prototyping. Furthermore, this integration ensures that all generated code is compilable, maintaining a seamless user experience and efficient task execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Leveraging ChatAtomicFlow to write a CodeGenerator Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pulling ChatAtomicFlow from FlowVerse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies from FlowVerse\n",
    "dependencies = [\n",
    "    {\"url\": \"aiflows/ChatFlowModule\", \"revision\": \"main\"},\n",
    "]\n",
    "from aiflows import flow_verse\n",
    "flow_verse.sync_dependencies(dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the CodeGenerator's Default Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining prompt templates\n",
    "\n",
    "#system prompt template\n",
    "system_prompt_template = \\\n",
    "\"\"\"\n",
    "You are a world class programmer that can complete any goal with code.\n",
    "      \n",
    "Your tasks are:\n",
    "    1. Write a python code in order to achieve a goal that is given to you. This code should be able to be run by the executor.\n",
    "\n",
    "Notice that:\n",
    "    1. If you use any external libraries, you must install them withing the code (in python)\n",
    "    2. All functions you write are modular, so make sure you make imports necessary imports within the function.\n",
    "    3. You must write docstrings for every function you write.\n",
    "\n",
    "    \n",
    "Your function will then be imported and called by an executor to finish the goal, you do not need to worry about the execution part.\n",
    "The executor will give you feedback on the code you write, and you can revise your code based on the feedback.\n",
    "\n",
    "Performance Evaluation:\n",
    "1. You must write your code in python\n",
    "2. Your answer must be able to be compiled and run by the executor (i.e, do not write codeblocks)\n",
    "\n",
    "**It's important that you should only respond in JSON format as described below:**\n",
    "Response Format:\n",
    "{\n",
    "    \"language_of_code\": \"language of the code\",\n",
    "    \"code\": \"String of the code and docstrings corresponding to the goal\",\n",
    "    \"finish\": \"True if you have finished the goal, False otherwise. Take in consideration the feedback you receive to decide if you have finished the goal or not\",\n",
    "}\n",
    "Ensure your responses can be parsed by Python json.loads\n",
    "    \n",
    "**It's important that the code you generate can be written by Python write, and is human-readable. The written file must also be indented and formatted, so that it is human-readable.**\n",
    "\"\"\"\n",
    "\n",
    "#init_human_prompt_template\n",
    "#This is the prompt passed to the ChatAtomicFlow the first time its called (i.e., the user asks to accomplish a goal)\n",
    "init_human_prompt_template = \\\n",
    "\"\"\"\n",
    "\"Here is the goal you need to achieve:\n",
    "{{goal}}\"\n",
    "\"\"\"\n",
    "\n",
    "#human_prompt_template\n",
    "#This is the prompt passed to the ChatAtomicFlow after the first time its called (i.e., feedback from the code interpreter)\n",
    "human_prompt_template = \\\n",
    "\"\"\"\n",
    "Here is the goal you need to achieve:\n",
    "{{goal}}\n",
    "Here is the previous code you have written:\n",
    "{{previous_code}}\n",
    "Here is the feedback from the previous code:\n",
    "{{feedback}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config_code_generator = {\n",
    "    \"_target_\": \"CodeGeneratorFlowModule.CodeGenerator.CodeGenerator.instantiate_from_default_config\",\n",
    "    \"name\": \"CodeGenerator\",\n",
    "    \"description\": \"Writes code with given instruction\",\n",
    "    \"backend\": {\n",
    "        \"api_infos\": \"???\"\n",
    "    },\n",
    "    \"model_name\": {\n",
    "        \"openai\": \"gpt-4\"\n",
    "    },\n",
    "    \"input_interface_non_initialized\": [\"goal\"],\n",
    "\n",
    "    \"input_interface_initialized\": [\"goal\", \"previous_code\", \"feedback\"],\n",
    "\n",
    "    \"output_interface\": [\"language_of_code\", \"code\"],\n",
    "\n",
    "    \"system_message_prompt_template\": {\n",
    "        \"template\": f'{system_prompt_template}'\n",
    "    },\n",
    "  \n",
    "    \"human_message_prompt_template\": {\n",
    "        \"template\": f'{human_prompt_template}',\n",
    "        \"input_variables\": [\"goal\", \"previous_code\", \"feedback\"],\n",
    "    },\n",
    "\n",
    "    \"init_human_message_prompt_template\": {\n",
    "        \"template\": f'{init_human_prompt_template}',\n",
    "        \"input_variables\": [\"goal\"]\n",
    "    },\n",
    "    #keep as context window only the first message (system prompt template) and the last two messages (last user-assistant interaction)\n",
    "    \"previous_messages\": {\n",
    "        \"first_k\": 1,\n",
    "        \"last_k\": 2,\n",
    "    }\n",
    "}\n",
    "dict_to_yaml(default_config_code_generator, \"CodeGeneratorFlowModule/CodeGenerator.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the CodeGenerator Flow\n",
    "\n",
    "**Requirements:** We want to make sure that the CodeGenerator generates a json parsable output. Implement this in the `CodeGenerator` class by requerying the LLM if it doesn't generate a parsable output (specifying to reformat its previous output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%compile_and_writefile CodeGeneratorFlowModule/CodeGenerator.py\n",
    "\n",
    "from aiflows.base_flows import AtomicFlow\n",
    "from aiflows.messages import FlowMessage\n",
    "from flow_modules.aiflows.ChatFlowModule import ChatAtomicFlow\n",
    "import json\n",
    "\n",
    "\n",
    "class CodeGenerator(ChatAtomicFlow):\n",
    "\n",
    "    def run(self, input_message: FlowMessage):\n",
    "        input_data = input_message.data\n",
    "        json_parsable = False\n",
    "        response = None\n",
    "        \n",
    "        #ensure the response is json parsable\n",
    "        while not json_parsable:\n",
    "            \n",
    "            output = self.query_llm(input_data=input_data).strip()\n",
    "            \n",
    "            try:\n",
    "                response = json.loads(output)\n",
    "                json_parsable = True\n",
    "            \n",
    "            except (json.decoder.JSONDecodeError, json.JSONDecodeError):\n",
    "                \n",
    "                ###### TODO: IF OUTPUT IN NOT JSON PARSABLE, REQUERY THE LLM \n",
    "                # HINT: Changing the input_data with feddback on the non json parsable response is a good way to go\n",
    "                ## Check out config above to see what input_data should have (you are querying the LLM for the second time (at least))\n",
    "                ## so look at human_message_prompt_template\n",
    "        ##package output message\n",
    "        reply = ???\n",
    "        ### send back reply\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Serving the CodeGenerator Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.serve_flow(\n",
    "    cl=cl,\n",
    "    flow_class_name=\"CodeGeneratorFlowModule.CodeGenerator.CodeGenerator\",\n",
    "    flow_endpoint=\"CodeGenerator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the CodeGenerator Flow:\n",
    "\n",
    "We will now test the CodeGenerator Flow by prompting the chatbot with a task and observing the generated code. We will use the following task as an example: \"Download Apple's stock prices, plot them, and save them in a PDF.\" The chatbot should generate code that accomplishes this task, and we will evaluate the generated code's quality and correctness. To showcase the limitations of using the code generator on its own, we will show that the code is not always compilable in your Python environment. To showcase we will manually uninstall, if you do have it installed, the `yfinance` package from the Python environment (which is a library the Flow will use in its answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall yfinance -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_yaml = os.path.join(\"CodeGeneratorFlowModule\", \"CodeGenerator.yaml\")\n",
    "default_config = read_yaml_file(path_to_yaml)\n",
    "overrides = copy.deepcopy(default_config)\n",
    "api_information = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(overrides, api_information)\n",
    "\n",
    "proxy_code_generator = serving.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"CodeGenerator\", #TODO: SPECIFY THE ENDPOINT\n",
    "    user_id=\"local\",\n",
    "    config_overrides=overrides,\n",
    ")\n",
    "data = {\n",
    "    \"goal\": \"Download Apple's stock price between 2015 and 2016, make a plot with it and save it as a pdf in './apple_stocks.pdf'\",\n",
    "}\n",
    "input_message = proxy_code_generator.package_input_message(data)\n",
    "future = proxy_code_generator.get_reply_future(input_message)\n",
    "reply_data = future.get_data()\n",
    "print(\"Data sent:\\n\",  data, \"\\n\")\n",
    "print(f'REPLY:\\n{reply_data[\"code\"]} \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you copy paste the reply from the chatbot, you will see that the code is not compilable. This is because the `yfinance` package is not installed in your Python environment. This is an example where the interpreter comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COPY PASTE YOU LLMS REPLY TO SEE IT DOES NOT RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using the Interperter Flow Module\n",
    "\n",
    "We already have an interpreter flow module implemented on the flow verse which you can checkout [here](https://huggingface.co/aiflows/InterpreterFlowModule) !\n",
    "\n",
    "Let's check out its config, serve it and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pulling the InterpreterFlowModule from FlowVerse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install dependencies from FlowVerse\n",
    "dependencies = [\n",
    "    {\"url\": \"aiflows/InterpreterFlowModule\", \"revision\": \"main\"},\n",
    "]\n",
    "from aiflows import flow_verse\n",
    "flow_verse.sync_dependencies(dependencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print it's default config\n",
    "\n",
    "interpreter_default_config = read_yaml_file(\"flow_modules/aiflows/InterpreterFlowModule/InterpreterAtomicFlow.yaml\")\n",
    "print(json.dumps(interpreter_default_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Serving the InterpreterFlowModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.serve_flow(\n",
    "    cl=cl,\n",
    "    flow_class_name=\"flow_modules.aiflows.InterpreterFlowModule.InterpreterAtomicFlow\",\n",
    "    flow_endpoint=\"InterpreterAtomicFlow\",\n",
    ")\n",
    "run_dispatch_worker_thread(cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the InterpreterFlowModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_code_interpreter = serving.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"InterpreterAtomicFlow\", #TODO: SPECIFY THE ENDPOINT\n",
    "    user_id=\"local\",\n",
    ")\n",
    "data = [\n",
    "    #should return \"Hello, World!\"\n",
    "    {\"language\": \"Python\", \"code\": \"print('Hello, World!')\"},\n",
    "    #should return an error\n",
    "    {\"language\": \"Python\", \"code\": \"import yfinance as yf\\nyf.download('AAPL', start='2015-01-01', end='2016-01-01')\"},\n",
    "]\n",
    "\n",
    "for dp in data:\n",
    "    input_message = proxy_code_interpreter.package_input_message(dp)\n",
    "    future = proxy_code_interpreter.get_reply_future(input_message)\n",
    "    reply_data = future.get_data()\n",
    "    print(\"Data sent:\\n\",  dp, \"\\n\")\n",
    "    print(\"REPLY:\\n\", reply_data, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Combining the CodeGenerator and Interpreter Flow Modules (ChatCodeInterpreter Flow)\n",
    "\n",
    "Now that we have both the CodeGenerator and Interpreter Flow Modules, we can combine them to create a ChatCodeInterpreter Flow. This flow will leverage the CodeGenerator to generate code based on user prompts, and the Interpreter to execute the generated code. The CodeGenerator will decide when to finish generating the code and return the final output (see system prompt template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the default config of the ChatCodeInterpreter Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_human_prompt_template = \\\n",
    "\"\"\"\n",
    "Here is the goal you need to achieve:\n",
    "{{goal}}\"\n",
    "Feedback:\n",
    "Do not finish this goal yet. You will receive feedback on the code you write.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config_ChatCodeInterpreter = \\\n",
    "{\n",
    "    \"name\": \"ChatCodeInterpreter\",\n",
    "    \"description\": \"A code which generate code with the help of a interpreter\",\n",
    "\n",
    "    # TODO: Define the target\n",
    "    \"_target_\": \"ChatCodeInterpreterFlowModule.ChatCodeInterpreter.ChatCodeInterpreter.instantiate_from_default_config\",\n",
    "\n",
    "    \"input_interface\": \"goal\",\n",
    "    \"output_interface\": [\"code\",\"interpreter_output\"],\n",
    "    \n",
    "    \"subflows_config\": {\n",
    "        \"Coder\": {\n",
    "            \"user_id\": \"local\",\n",
    "            \"flow_endpoint\": \"CodeGenerator\",\n",
    "            \"name\": \"Proxy of Coder\",\n",
    "            \"description\": \"A proxy flow of the Coder flow.\",\n",
    "             \"backend\": {\n",
    "                \"api_infos\": \"???\",\n",
    "                \"model_name\": {\"openai\": \"gpt-4\"}\n",
    "            },\n",
    "             \"init_human_message_prompt_template\": {\n",
    "                \"template\": f'{init_human_prompt_template}',\n",
    "                \"input_variables\": [\"goal\"]\n",
    "             }\n",
    "        },\n",
    "        \"Interpreter\": {\n",
    "           \n",
    "            \"user_id\": \"local\",\n",
    "            \n",
    "            \"flow_endpoint\": \"InterpreterAtomicFlow\",\n",
    "            \"name\": \"Proxy of Interpreter Flow\",\n",
    "            \"description\": \"A proxy flow of the Interpreter Flow.\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "dict_to_yaml(default_config_ChatCodeInterpreter, \"ChatCodeInterpreterFlowModule/ChatCodeInterpreter.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the ChatCodeInterpreter Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%compile_and_writefile ChatCodeInterpreterFlowModule/ChatCodeInterpreter.py\n",
    "\n",
    "\n",
    "######## TODO WRITE THE ChatCodeInterpreter Class #######\n",
    "####### HINT: TAKE INSPIRATION FROM THE PREVIOUS TUTORIAL \n",
    "####### (4.ChatWithGuards --> The ChatWithPIGuard and ChatQueryDBwithPIGuard Flows)\n",
    "from aiflows.base_flows import CompositeFlow\n",
    "from aiflows.messages import FlowMessage\n",
    "from aiflows.interfaces import KeyInterface\n",
    "\n",
    "class ChatCodeInterpreter(CompositeFlow):\n",
    "        \n",
    "    def __init__(self,**kwargs):\n",
    "        ???\n",
    "        \n",
    "    def set_up_flow_state(self):\n",
    "        \"\"\" Set up the flow state. (called in __init__)\"\"\"\n",
    "        super().set_up_flow_state()\n",
    "        \n",
    "    def determine_current_state(self):\n",
    "        \"\"\" Determine the current state of the flow.\"\"\"\n",
    "        ???\n",
    "                        \n",
    "    def call_coder(self):\n",
    "        \"\"\" Call the coder subflow.\"\"\"\n",
    "        ???\n",
    "        \n",
    "    def call_interpreter(self):\n",
    "        \"\"\" Call the interpreter subflow.\"\"\"\n",
    "        ???\n",
    "        \n",
    "    def generate_reply(self):\n",
    "        \"\"\" Answers back to the original input message\"\"\"  \n",
    "        ???\n",
    "        \n",
    "    def register_data_to_state(self, input_message):\n",
    "        \"\"\" Register the data from the input message to the flow state.\"\"\"\n",
    "        ????\n",
    "\n",
    "   \n",
    "    def run(self, input_message: FlowMessage):\n",
    "        \"\"\" Run the flow.\"\"\"\n",
    "        ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Serve the ChatCodeInterpreter Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving.serve_flow(\n",
    "    cl=cl,\n",
    "    flow_class_name=\"ChatCodeInterpreterFlowModule.ChatCodeInterpreter.ChatCodeInterpreter\",\n",
    "    flow_endpoint=\"ChatCodeInterpreter\",\n",
    ")\n",
    "\n",
    "run_dispatch_worker_thread(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure we don't have yfinacne installed\n",
    "!pip uninstall yfinance -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the ChatCodeInterpreter Flow\n",
    "\n",
    "Now let's test the ChatCodeInterpreter Flow by prompting the chatbot with the same task as before: \"Download Apple's stock prices, plot them, and save them in a PDF.\" The chatbot should generate code that accomplishes this task, and the interpreter should execute the generated code (including installing the necessary packages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "default_config = default_config_ChatCodeInterpreter\n",
    "overrides = copy.deepcopy(default_config)\n",
    "api_information = [ApiInfo(backend_used=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))]\n",
    "quick_load_api_keys(overrides, api_information)\n",
    "\n",
    "proxy_code_generator = serving.get_flow_instance(\n",
    "    cl=cl,\n",
    "    flow_endpoint=\"ChatCodeInterpreter\", #TODO: SPECIFY THE ENDPOINT\n",
    "    user_id=\"local\",\n",
    "    config_overrides=overrides,\n",
    ")\n",
    "data = [\n",
    "    {\"id\": 0, \"goal\": \"Download Apple's stock price between 2015 and 2016, make a plot with it, \\\n",
    "        and save it as a pdf in Download Apple's stock price between 2015 and 2016, make a plot with \\\n",
    "            it and save it as a pdf in 'apple_stocks.pdf'\"},\n",
    "]\n",
    "\n",
    "for dp in data:\n",
    "    input_message = proxy_code_generator.package_input_message(dp)\n",
    "    future = proxy_code_generator.get_reply_future(input_message)\n",
    "    reply_data = future.get_data()\n",
    "    print(\"Data sent:\\n\",  dp, \"\\n\")\n",
    "    print(\"REPLY:\\n\", reply_data, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how a pdf named `apple_stock_prices.pdf` is saved in the current directory. This is the pdf that was generated by the ChatAtomicFlow executerd by the interpreter Flow.\n",
    "\n",
    "You can also check out the code it generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reply_data[\"code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you copy paste the code in a new cell it should also run since it installed the necessary packages !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COPY PASTE YOUR CODE (IT SHOULD RUN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coflows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
