name: ChatWithPIRails
description: A sequential flow that calls a safeguard flow and then a chatbot flow.         The
  safeguard flow checks for prompt injections.
_target_: ChatWithPIGuardFlowModule.ChatWithPIGuard.ChatWithPIGuard.instantiate_from_default_config
input_interface: question
output_interface: answer
subflows_config:
  Safeguard:
    user_id: local
    flow_endpoint: PromptInjectionDetectorFlow
    name: Proxy of PromptInjectionDetectorFlow.
    description: A proxy flow that checks for prompt injections.
  ChatBot:
    user_id: local
    flow_endpoint: ChatAtomicFlow
    name: Proxy of Chat Flow
    backend:
      api_infos: ???
      model_name:
        openai: gpt-4
    input_interface: question
    input_interface_non_initialized: question
    description: A proxy flow that calls an LLM model to generate a response, if the
      prompt is valid (no injection).
    system_message_prompt_template:
      template: You are a helpful chatbot that truthfully answers questions
    init_human_message_prompt_template:
      template: 'Answer the following question: {{question}}'
      input_variables:
      - question
    human_message_prompt_template:
      template: 'Answer the following question: {{question}}'
      input_variables:
      - question
