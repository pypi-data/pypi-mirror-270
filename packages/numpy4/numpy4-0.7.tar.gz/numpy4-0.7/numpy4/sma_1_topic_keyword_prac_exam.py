# -*- coding: utf-8 -*-
"""sma_1_topic_keyword_prac_exam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P2NQIs38N6L1ibkP7x4YgzBv6g5r6d_T
"""

!pip install pyLDAvis
!pip install nltk

# '''
# This program first loads the social media data from a CSV file and removes any rows with missing values. It then uses the langdetect library to remove any non-English content, and performs text preprocessing by removing stopwords and performing lemmatization using the NLTK library.

# Next, it creates a document-term matrix using TF-IDF vectorization, and converts it to a gensim corpus. It then uses the LdaModel class from gensim to perform LDA topic modeling on the corpus, specifying the number of topics to be 5.

# Finally, it visualizes the topics using the pyLDAvis library, which generates an interactive visualization of the topics. You can then use this visualization to explore the topics and their associated words and documents.

# '''

# import nltk
# from nltk.stem import WordNetLemmatizer
# from nltk.corpus import stopwords
# import csv
# import numpy as np
# import pandas as pd
# # from langdetect import detect
# from textblob import TextBlob
# from sklearn.feature_extraction.text import TfidfVectorizer
# from gensim import corpora, models
# import pyLDAvis.gensim

# nltk.download('stopwords')
# nltk.download('punkt')
# nltk.download('wordnet')

# # Load social media data from a CSV file
# data = pd.read_csv("/content/drive/MyDrive/SMA_datasets_prac_exam/social_media_data.csv")

# # Remove any rows with missing values
# data = data.dropna()

# # Remove any non-English content
# # data['language'] = data['text'].apply(lambda x: detect(x))
# # data = data[data['language'] == 'en']

# # Clean up the text by removing stopwords and performing lemmatization
# stop_words = stopwords.words('english')
# lemmatizer = WordNetLemmatizer()


# def preprocess(text):
#     words = TextBlob(text).words.singularize()
#     words = [lemmatizer.lemmatize(word)
#              for word in words if word not in stop_words]
#     return ' '.join(words)


# data['clean_text'] = data['text'].apply(preprocess)

# # Create a document-term matrix using TF-IDF vectorization
# vectorizer = TfidfVectorizer()
# doc_term_matrix = vectorizer.fit_transform(data['clean_text'])

# # Convert the document-term matrix to a gensim corpus
# corpus = corpora.MmCorpus(doc_term_matrix.transpose())

# # Perform LDA topic modeling on the corpus
# lda_model = models.LdaModel(
#     corpus=corpus, num_topics=5, id2word=vectorizer.get_feature_names())

# # Visualize the topics using pyLDAvis
# pyLDAvis.enable_notebook()
# vis = pyLDAvis.gensim.prepare(lda_model, corpus, vectorizer)
# pyLDAvis.display(vis)

# '''
# The output of the above code would be an interactive visualization of the topics generated by the LDA model using the pyLDAvis library. The visualization would show the following:

# A two-dimensional plot of the topics, where each circle represents a topic and the size of the circle represents the prevalence of the topic in the corpus.
# A list of the top 30 most relevant terms for each topic, where relevance is calculated as a combination of the term frequency and the exclusivity of the term to the topic.
# A horizontal bar chart that shows the frequency of each term in the corpus, along with its overall frequency across all topics.
# '''

# import pandas as pd
# import numpy as np
# import nltk
# from textblob import TextBlob
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# from sklearn.feature_extraction.text import TfidfVectorizer
# from gensim import corpora, models
# import matplotlib.pyplot as plt

# nltk.download('stopwords')  # Required for English stopwords
# nltk.download('wordnet')  # Required for WordNet lemmatizer
# nltk.download('punkt')

# # Load social media data from CSV
# data = pd.read_csv("/content/drive/MyDrive/SMA_datasets_prac_exam/social_media_data.csv")

# # Remove missing values
# data.dropna(inplace=True)

# # Define text preprocessing functions
# lemmatizer = WordNetLemmatizer()
# stop_words = set(stopwords.words('english'))

# def preprocess(text):
#     # Tokenize, remove stopwords, and lemmatize
#     words = TextBlob(text).words.singularize()
#     words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
#     return ' '.join(words)

# # Clean the text data
# data['clean_text'] = data['text'].apply(preprocess)

# # Create a document-term matrix using TF-IDF
# vectorizer = TfidfVectorizer()
# doc_term_matrix = vectorizer.fit_transform(data['clean_text'])

# # Get feature names using the updated method
# feature_names = vectorizer.get_feature_names_out()

# # Convert document-term matrix to Gensim corpus
# corpus = [dict([(feature_names[i], freq) for i, freq in enumerate(row) if freq > 0]) for row in doc_term_matrix.toarray()]
# dictionary = corpora.Dictionary([list(corpus[i].keys()) for i in range(len(corpus))])

# # Build the LDA model
# lda_model = models.LdaModel(corpus=[dictionary.doc2bow(row.keys()) for row in corpus], num_topics=5, id2word=dictionary)

# # Analyze the LDA topics
# for idx, topic in enumerate(lda_model.print_topics(num_words=10)):
#     print(f"Topic {idx + 1}: {topic}")


# # Display the topics in a more structured format
# topics = lda_model.print_topics(num_words=10)

# print("Identified Topics from LDA:")
# for idx, topic in enumerate(topics):
#     topic_number = idx + 1  # Topic number starts from 1
#     words_and_weights = topic[1].split(" + ")
#     print(f"\nTopic {topic_number}:")
#     for item in words_and_weights:
#         word, weight = item.split("*")
#         print(f"  - {word.strip()} (Weight: {weight.strip()})")


# for idx, topic in enumerate(lda_model.show_topics(num_topics=5, formatted=False)):
#     topic_terms = [term[0] for term in topic[1]]
#     term_weights = [term[1] for term in topic[1]]

#     plt.figure()
#     plt.bar(topic_terms, term_weights, color='skyblue')
#     plt.title(f"Topic {idx + 1}")
#     plt.xlabel("Terms")
#     plt.ylabel("Weights")
#     plt.show()

# # # Perform LDA topic modeling
# # lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary)

# # # Visualize the topics with pyLDAvis
# # pyLDAvis.enable_notebook()
# # vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)
# # pyLDAvis.display(vis)

#Content Analysis

import nltk
import pandas as pd
import gensim
from gensim import corpora
from gensim.models import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter

# Read data from CSV file
df = pd.read_csv('/content/drive/MyDrive/SMA_datasets_prac_exam/social_media_data.csv')

# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('punkt')

# Tokenization, stopword removal, and lemmatization
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]
    return filtered_tokens

df['tokens'] = df['text'].apply(preprocess_text)

# Create dictionary and document-term matrix
dictionary = corpora.Dictionary(df['tokens'])
corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]

# Topic modeling using LDA
num_topics = 5  # Adjust the number of topics as needed
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20)

# Print topics
print("Topics:")
for topic_id in range(num_topics):
    print(f"Topic {topic_id + 1}: {lda_model.print_topic(topic_id)}")

# Keyword extraction
all_tokens = [token for tokens in df['tokens'] for token in tokens]
keyword_counter = Counter(all_tokens)

# Print most common keywords
num_keywords = 10  # Adjust the number of keywords as needed
print("\nTop Keywords:")
for keyword, count in keyword_counter.most_common(num_keywords):
    print(f"{keyword}: {count}")

# Display topics in a structured format
print("Identified Topics from LDA:")
for idx in range(num_topics):
    topic_info = lda_model.print_topic(idx, topn=10)  # Get the top 10 terms per topic
    topic_number = idx + 1
    words_and_weights = topic_info.split(" + ")
    print(f"\nTopic {topic_number}:")
    for item in words_and_weights:
        weight, word = item.split("*")
        print(f"  - {word.strip()} (Weight: {weight.strip()})")

# visualization
for idx, topic in enumerate(lda_model.show_topics(num_topics, formatted=False)):
    topic_terms = [term[0] for term in topic[1]]
    term_weights = [term[1] for term in topic[1]]

    plt.figure()
    plt.bar(topic_terms, term_weights, color='skyblue')
    plt.title(f"Topic {idx + 1}")
    plt.xlabel("Terms")
    plt.ylabel("Weights")
    plt.show()