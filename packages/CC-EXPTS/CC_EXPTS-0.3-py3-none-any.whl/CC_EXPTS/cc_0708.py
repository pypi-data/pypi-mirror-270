# # -*- coding: utf-8 -*-
# """CC_07&08.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1n9YMZfjIW9-l_OYyeMeRvyHlCe7lepWc

# ##Practical 07
# Feature Extraction using PCA
# """

# # Data Collection and Preprocessing
# from sklearn.datasets import load_iris

# # Load the Iris dataset
# iris = load_iris()
# X = iris.data
# y = iris.target

# # Preprocessing: Standardize the features
# from sklearn.preprocessing import StandardScaler

# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# # Feature Selection and Extraction: Apply PCA
# from sklearn.decomposition import PCA

# pca = PCA(n_components=2) # Reduce to 2 components
# X_pca = pca.fit_transform(X_scaled)

# # Visualization
# import matplotlib.pyplot as plt

# plt.figure(figsize=(10, 6))
# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
# plt.xlabel('Principal Component 1')
# plt.ylabel('Principal Component 2')
# plt.title('Scatter Plot of Observations in 2D PCA Space')
# plt.grid(True)
# plt.show()

# import numpy as np

# explained_variance_ratio = pca.explained_variance_ratio_
# print("Explained Variance Ratio:", explained_variance_ratio)

# cumulative_variance_ratio = np.cumsum(explained_variance_ratio)
# print("Cumulative Explained Variance Ratio:", cumulative_variance_ratio)

# """##Practical 08
# Psychological Model
# """

# !pip install -U sentence-transformers

# import nltk
# nltk.download('punkt')
# nltk.download('stopwords')

# import nltk
# from nltk.tokenize import word_tokenize
# from nltk.corpus import stopwords

# # Step 1: Encoding - Tokenization
# def encode_text(text):
#     # Tokenize the text
#     tokens = word_tokenize(text)
#     return tokens

# # Step 2: Storage - Removing Stopwords
# def store_text(tokens):
#     # Remove stopwords
#     stop_words = set(stopwords.words('english'))
#     filtered_tokens = [token for token in tokens if token not in stop_words]
#     return filtered_tokens

# # Step 3: Retrieval - Simulating a simple retrieval task
# def retrieve_text(filtered_tokens):
#     # For demonstration, let's say we want to find the frequency of a specific word
#     word_to_find = str(input("Enter word to find : "))
#     word_frequency = filtered_tokens.count(word_to_find)
#     return word_frequency

# # Step 4: Transformation - Simulating a simple transformation task
# def transform_text(word_frequency):
#     # For demonstration, let's say we want to categorize the word frequency
#     if word_frequency > 5:
#         return "High frequency"
#     else:
#         return "Low frequency"

# # Example text
# text = str(input("Enter sentence : "))

# # Implementing the Information Processing Model
# encoded_text = encode_text(text)
# stored_text = store_text(encoded_text)
# retrieved_text = retrieve_text(stored_text)
# transformed_text = transform_text(retrieved_text)

# print("Encoded Text:", encoded_text)
# print("Stored Text:", stored_text)
# print("Retrieved Text:", retrieved_text)
# print("Transformed Text:", transformed_text)